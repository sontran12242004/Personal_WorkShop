[{"uri":"https://thienluhoan.github.io/workshop-template/3-blogstranslated/3.1-blog1/","title":"Blog 1","tags":[],"description":"","content":"How Novo Nordisk, Columbia University and AWS collaborated with the OpenFold AI Consortium to develop OpenFold3 by Daniele Granata, Anamaria Todor, Arthur Grabman, Rômulo Jales, Jacob Mevorach, and Stig Bøgelund Nielsen | on 03 JUN 2025 in | Customer Solution | High Performance Computing | Life Sciences Permalink\nThis post was contributed by Daniele Granata, Principal Modelling Scientist; Søren Moos, Architect Advisor Director; Rômulo Jales, Senior Software Engineer; Stig Bøgelund Nielsen, Senior HPC Cloud Engineer at Novo Nordisk; and Jake Mevorach, Senior Specialist, Application Modernization; Arthur Grabman, Principal Technical Account Manager; and Anamaria Todor, Principal Solutions Architect at AWS.\nIn this blog post, we are excited to share how Novo Nordisk, Columbia University, and AWS collaborated to develop OpenFold3, an advanced protein structure prediction model, using scalable and cost-effective bioinformatics solutions on AWS.\nThe OpenFold Consortium, a non-profit AI research and development group, has been at the forefront of creating open-source tools for biology and drug development. Their latest project, OpenFold3, represents a significant advancement in protein structure prediction. This post will explore how we leveraged AWS services and innovative approaches to overcome challenges in training this complex model while maintaining cost efficiency and scalability.\nFirst, we will introduce protein structure prediction and discuss its importance in advancing medical research and pharmaceutical development.\nNext, we will dive into three main challenges we faced and how we addressed them:\nCreating a lightweight research environment: We will discuss how we developed the Research Collaboration Platform (RCP) using AWS services, providing a secure, flexible, and efficient workspace for collaborative efforts.\nCreating Multiple Sequence Alignments (MSAs): We will explore how we optimized the MSA generation process using AWS Graviton processors, achieving significant improvements in both speed and cost-efficiency.\nAI/ML Training: We will present our approach to the computationally intensive task of training the OpenFold3 model, including the use of Amazon EC2 Capacity Blocks for ML and Spot Instances to efficiently manage resources.\nThis post provides valuable insights for researchers, data scientists, and organizations looking to leverage cloud computing for complex bioinformatics tasks. Whether you\u0026rsquo;re working on protein structure prediction or other computationally intensive projects in life sciences, the strategies and solutions discussed can help you optimize workflows for both performance and cost-effectiveness.\nProtein Structure Prediction – Overview If you\u0026rsquo;re interested in curing diseases, advancing medicine, and even saving lives, a great place to start is understanding protein structures. There are many different types of proteins in the human body. Some proteins are beneficial, such as the lactase enzyme, which helps digest dairy products. Other proteins can be harmful and lead to disease or even death. Even small changes in the structure of these microscopic compounds can determine survival.\nProtein-based therapeutics is a new and promising approach to treating diseases. However, because proteins have a strong impact on the human body, we need to ensure that we create proteins with the appropriate structure to treat diseases.\nThe traditional process of developing protein-based therapeutics typically includes:\nFinding receptors or other structures in the body (in the industry called \u0026ldquo;targets\u0026rdquo;) that you believe a protein can bind to achieve the desired therapeutic effect. Developing a process to produce a protein capable of acting on this target. Verifying through methods such as X-ray crystallography or Nuclear Magnetic Resonance (NMR) Spectroscopy that the production process creates a protein with the desired structure. Conducting extensive research and testing, ultimately clinical trials, to ensure the newly developed protein achieves the desired effect. Historically, the biggest problem was that while we could genetically engineer proteins with specific sequences, we lacked good tools to predict protein structure from genetic data. This made the therapeutic development process time-consuming, because even when you know how to create a protein with a specific genetic sequence, you still need many highly trained personnel operating expensive equipment—even by global pharmaceutical company standards—to verify the structure. If the structure is incorrect, you must start over and repeat the process until the correct structure is achieved. The process must be restarted each time a new molecule needs to be characterized.\nAll of this makes developing protein-based therapeutics extremely expensive and time-consuming. A breakthrough emerged when researchers realized that AI could be applied to significantly accelerate this process. Researchers had large amounts of historical data, combining protein structures with corresponding genetic data. The question was posed:\n\u0026ldquo;If we have data combining protein structures with genetic data, can we use it to train a model that, when provided with a new genetic sequence, will predict the corresponding protein structure?\u0026rdquo;\nThe research community answered \u0026ldquo;yes,\u0026rdquo; and from that point, many AI models emerged that take genetic sequences as input and output predicted protein structures.\nRunning these models requires only a small amount of electrical power but provides crucial feedback about protein behavior before starting expensive and time-consuming experiments.\nIntroducing OpenFold3 The OpenFold Consortium is a non-profit AI research and development consortium, specializing in developing free open-source tools to support biology and drug discovery. One of the tools they develop is OpenFold, an AI model that takes genetic sequences as input and outputs predicted protein structures.\nOpenFold3 is the latest version of this software, and to accelerate its development, Novo Nordisk and AWS collaborated with the OpenFold Consortium and Columbia University.\nFirst Challenge: An Optimal Research Environment We started this project with limited time and budget, and the first challenge the project faced was that researchers needed a lightweight research environment as soon as possible.\nAs the priority scientific partner, Novo Nordisk needed to create an environment that could fundamentally change how research collaboration works. This environment had to enable seamless collaboration between Novo Nordisk, Columbia University, and the OpenFold Consortium, while strictly adhering to security and compliance standards.\nIt was important that researchers could use their preferred tools and devices, maintaining established workflows without disruption. With a tight timeline, we needed rapid setup capability and short infrastructure build times, combined with a streamlined onboarding process for external collaborators.\nThe system also needed to be flexible enough to meet different consortium contract terms, particularly regarding geographic data location and access controls. Most importantly, this environment had to accelerate early target discovery by providing robust computational resources while maintaining cost efficiency. Finally, an environment was needed that could dynamically scale up during intensive training phases but could scale down when not in use to maintain cost efficiency.\nAll of these requirements had to be balanced with empowering researchers with creative freedom—a factor particularly important when considering the high computational requirements for training large language models for protein structure prediction.\nFigure 1 – This diagram illustrates Novo Nordisk\u0026rsquo;s Research Collaboration Platform (RCP), showing how the platform creates a secure and high-performance research environment on AWS Cloud. Key elements to note are: (1) Secure access layer using Okta authentication, allowing researchers to connect securely via web portals or SSH.\n(2) Flexible computing resources managed by AWS ParallelCluster, automatically scaling to match researcher needs. (3) AWS EFS and AWS FSx file systems handle research data. The key point is that this architecture allows scientists to conduct complex research securely and efficiently, especially for high-demand tasks like AI model training, while ensuring regulatory compliance.\nThis platform stands out by combining enterprise-grade security with the flexibility researchers need, and can be deployed in less than 2 hours through automation.\nTo meet these requirements, Novo Nordisk developed the Research Collaboration Platform (RCP), a comprehensive solution built on AWS, providing a secure and flexible environment for computational research. The architecture (as illustrated in Figure 1) uses AWS ParallelCluster as the foundation, operating within a Virtual Private Cloud (VPC) to ensure secure isolation for research workloads.\nAt the center of the RCP security model is integration with Okta, providing robust user directory services and multi-factor authentication. Researchers can access the platform through web portals or SSH using Okta Advanced Server Access, ensuring secure and controlled access to resources. The platform entry point is an EC2 head node located in a public subnet, containing necessary research tools including RStudio, Docker containers, and scientific frameworks like Singularity.\nThe compute infrastructure is intelligently designed with SLURM job scheduler, managing workloads across different instance types, optimized for diverse computational needs—from general-purpose compute to memory-intensive tasks and GPU acceleration. Amazon EC2 Auto Scaling ensures resources are allocated dynamically based on demand, helping maintain cost efficiency while meeting computational requirements.\nThe platform uses Amazon Aurora Serverless for database management and Amazon DynamoDB for cluster state information, providing reliable and scalable data storage solutions.\nData management is handled through a combination of Amazon EFS and FSx for Lustre, providing both general-purpose and high-performance file storage. The entire infrastructure is monitored by Amazon CloudWatch, with AWS CloudFormation enabling infrastructure as code, ensuring consistent and repeatable deployments. Deploying an RCP instance takes less than 2 hours.\nThis architecture provides researchers with flexibility to use their preferred tools while ensuring security and compliance requirements. The combination of containerization, automated scaling, and high-performance computing capabilities enables rapid experimentation and accelerates research workflows, especially for computationally intensive tasks like training the OpenFold3 model.\nSecond Challenge: The MSA Generation Process With the research environment deployed and operational, we had to find a way to generate millions of Multiple Sequence Alignments (MSAs). A multiple sequence alignment (MSA) is essentially a unit of information describing the degree of similarity between three or more genetic sequences. This is important because these MSAs (along with other data) will be used to train OpenFold3.\nThe software we were tasked with scaling to generate these MSAs is HHBlits (specifically version 3, available for free on GitHub). When calculating cost and runtime for this step, we found it would take twice the time and more than double the cost compared to the expected budget and timeline if using the initial compute instances, r5.16xlarge.\nAmazon EC2 R8g instances, running on the latest generation AWS Graviton4 processors, had the potential to provide better price performance for this memory-intensive workload. Benchmark testing on r8g.16xlarge showed 50% lower runtime and 55% lower cost for HHBlits compared to r5.16xlarge.\nCombined with some other optimizations, the entire process ran much faster at lower cost, allowing us to generate over one million MSAs per day.\nOne challenge when transitioning from R5 to R8g was the difference in CPU architecture. R8g uses ARM-based CPUs, while R5 is x86-64. In terms of source code, this was not an issue because all code is C, C++, and Python, supporting both AArch64 and x86-64. However, we had to create an entirely new cluster because ParallelCluster requires head nodes and compute nodes to have the same CPU type.\nWe solved this by defining a new AWS ParallelCluster configuration, synchronizing CPU architecture between head nodes and compute nodes. To improve performance, we also customized the EC2 image with EBS as local storage. All generated MSAs were stored directly into an Amazon S3 bucket immediately. This configuration proved to be very efficient.\nFrom the end-user perspective, there was very little change. Scientists only needed to use a different host name to log in and access the Graviton Slurm queues. We also reused the same EFS and FSx for Lustre filesystems configured initially without performance penalties.\nThe best aspect of this architecture is its high scalability. If we want to adjust the number of MSAs generated simultaneously, we simply add or remove nodes. The figure of one million MSAs mentioned above is not the maximum limit; in fact, we don\u0026rsquo;t yet know the upper limit for throughput when using this method.\nThird Challenge: AI/ML Training As is known, the key factor for running AI/ML workloads is GPUs. But for protein prediction, GPUs alone are not enough. The system must also meet a range of prerequisites including petabyte-scale storage, high-speed networking, and hundreds of GPUs.\nThe OpenFold3 project required up to 256 GPUs for training. This equates to 32 p5en.48xlarge EC2 instances running in parallel, processing and generating data over several weeks, 24 hours a day.\nThis configuration requires infrastructure that is scalable, resilient, observable, and most importantly, flexible.\nInitially, scientists needed to build training logic almost from scratch. The level of uncertainty was high; therefore, 32 GPU machines were not necessary from the start. What they needed was an environment to develop, conduct small experiments, and gradually scale up the number of nodes used for training. This is exactly the infrastructure capability that AWS ParallelCluster can provide.\nWhen training OpenFold3 on the generated data, we faced three main challenges:\nBudget constraints required a cost-effective solution, avoiding costly errors. Increasing model complexity required distributed AI/ML training infrastructure, as the model no longer fits on a single GPU or instance. Securing large GPU resources for both training and preliminary testing required quick action and coordination among all stakeholders. To address these challenges, we implemented the following solutions:\nGPU Resource Allocation: We used Amazon EC2 Capacity Blocks for ML, a consumption model that allows reservation of high-performance GPUs for short periods for machine learning workloads. This service allows users to reserve hundreds of NVIDIA GPUs synchronously in Amazon EC2 UltraClusters, specifying cluster size, future start date, and duration. Capacity Blocks provide reliable, predictable GPU access without long-term commitments, ideal for training and fine-tuning ML models, running experiments, and preparing for future scaling needs. Pricing is dynamic, based on supply and demand, typically around P5 On-Demand rates.\nCost Optimization: To optimize costs, we used Amazon EC2 Spot instances. Spot instances allow use of spare EC2 capacity with discounts up to 90% compared to On-Demand pricing for fault-tolerant and flexible workloads. As AWS GPU infrastructure expands, many new GPU instance types are available through the Spot pricing model, enabling large-scale workloads to run at significantly lower costs thanks to AWS\u0026rsquo;s massive operational scale. Spot instances can be reclaimed by AWS when EC2 needs resources, typically with a two-minute interruption notice, helping applications handle transitions gracefully.\nBy implementing these methods, Novo Nordisk\u0026rsquo;s RCP team achieved an 85% cost reduction compared to using On-Demand instances for our use case.\nDistributed Training Infrastructure: We drew inspiration from AWSome Distributed Training examples, particularly those about DeepSpeed, to build our scalable training infrastructure. This configuration efficiently uses GPUs from multiple instances simultaneously, creating a highly effective and scalable system. The final infrastructure allows Slurm job submissions to an autoscaling, efficient, distributed GPU cluster, capable of handling the complex task of training the new OpenFold3 model.\nBy implementing these solutions, we successfully overcame the initial challenges, creating a cost-effective, scalable, and efficient training environment for OpenFold3.\nConclusion The successful training of OpenFold3 represents an important collaboration between Novo Nordisk, Columbia University, and AWS, overcoming three major challenges through innovative solutions:\nNovo Nordisk developed the Research Collaboration Platform (RCP), a secure and flexible research environment built on AWS, enabling seamless collaboration while maintaining strict security standards. The platform can be deployed in less than two hours and provides dynamic scaling for computational resources.\nTogether, we optimized the Multiple Sequence Alignment (MSA) generation process using AWS Graviton processors, achieving a 50% reduction in runtime and a 55% cost reduction. This enabled researchers to generate over one million MSAs per day with a highly scalable architecture.\nTogether, we addressed AI/ML training challenges by combining Amazon EC2 Capacity Blocks for ML and Spot instances. Our distributed training infrastructure efficiently utilized 256 GPUs across 32 p5en.48xlarge EC2 machines.\nThese solutions not only made the project cost-effective and efficient but also created a blueprint for future large-scale bioinformatics collaborations between Novo Nordisk and AWS.\nThe successful development of OpenFold3 advances the field of protein structure prediction, contributing to faster and more efficient therapeutic development processes.\nTAGS: Protein Folding\nAbout the Authors Daniele Granata Daniele Granata is a Principal Modelling Scientist in the In Silico Protein Discovery field at Novo Nordisk. He is passionate about building connections between people and technology and working at the intersection of protein science, AI, and HPC. He holds a bachelor\u0026rsquo;s and master\u0026rsquo;s degree in Physics, a PhD in \u0026ldquo;Physics and Chemistry of Biological Systems,\u0026rdquo; and 5 years of postdoctoral experience in the US and Denmark.\nOver the past 6 years at Novo Nordisk, Daniele has been prominent in advancing data science and generative AI to design new drugs for patients, while maintaining a passion for open sourcing for the scientific community.\nAnamaria Todor Anamaria Todor is a Principal Solutions Architect based in Copenhagen, Denmark. She first encountered computers at the age of 4 and has never left computer science, video games, and engineering since. She has worked in various technical roles, from freelancer, full-stack developer, data engineer, technical lead, to CTO, at various companies in Denmark, focusing on the gaming and advertising industries. She has worked at AWS for over 5 years as a Principal Solutions Architect, primarily focusing on life sciences and AI/ML. Anamaria holds a bachelor\u0026rsquo;s degree in Applied Engineering and Computer Science, a master\u0026rsquo;s in Computer Science, and over 10 years of experience with AWS. When not working or playing video games, she coaches girls and women professionals to help them understand and find their path in the technology field.\nArthur Grabman Arthur Grabman is a Principal Technical Account Manager with over 15 years of experience, specializing in HPC for enterprise clients. He is passionate about translating complex technical concepts into concrete business value. Outside of work, Arthur enjoys traveling with his family.\nRômulo Jales Rômulo Jales is a Senior Software Engineer at Novo Nordisk. With a solid foundation of over 15 years in software development, Rômulo enjoys completing work by creating simple and elegant solutions, even in critical mission applications.\nRômulo holds a bachelor\u0026rsquo;s degree in Computer Engineering and is still driven by the initial curiosity about how and why things work the way they do. For him, there is always something new to learn and discover.\nJacob Mevorach Jacob Mevorach is a Senior Specialist in containers for healthcare and life sciences at AWS. Jacob has a background in bioinformatics and machine learning. Before joining AWS, Jacob focused on supporting and implementing large-scale analyses for genomics and other scientific fields.\nStig Bøgelund Nielsen Stig Bøgelund Nielsen is a Senior Cloud \u0026amp; HPC Engineer at Novo Nordisk. A lifelong engagement with IT, starting from supporting local businesses, has shaped his career. Over 8 years at IBM provided a solid foundation, with roles such as IT Service and Infrastructure Architect, Team Lead, and Transition \u0026amp; Transformation Architect/Project Manager, all focused on creating business value through effective IT strategies and implementations.\nHe joined Novo Nordisk nearly two years ago as a Cloud \u0026amp; HPC Engineer and is currently the Tech Lead of the Research Collaboration Platform (RCP), directly supporting pioneering research, including the OpenFold project. He is passionate about leveraging technology to drive research and innovation.\n"},{"uri":"https://thienluhoan.github.io/workshop-template/3-blogstranslated/3.2-blog2/","title":"Blog 2","tags":[],"description":"","content":"Accelerate Amazon Redshift Data Lake queries with AWS Glue Data Catalog Column Statistics by Kalaiselvi Kamaraj, Asser Moustafa, and Mark Lyons | on 01 OCT 2024 in | Amazon Redshift | Amazon Simple Storage Service (S3) | Analytics, Announcements | AWS Big Data | AWS Glue | Best Practices Permalink\nAmazon Redshift allows you to query and retrieve structured and semi-structured data from open format files in Amazon S3 data lakes without needing to load data into Amazon Redshift tables. Amazon Redshift extends SQL capabilities to the data lake, enabling you to run analytical queries. Redshift supports many different table data formats such as CSV, JSON, Parquet, ORC, and open table formats like Apache Hudi, Linux Foundation Delta Lake, and Apache Iceberg.\nYou create Redshift external tables by defining the structure for files, the S3 location of files, and registering them as tables in an external data catalog. The external data catalog can be AWS Glue Data Catalog, the data catalog that comes with Amazon Athena, or your Apache Hive metastore.\nOver the past year, Amazon Redshift has added several performance optimizations for data lake queries across many aspects of the query engine such as rewrite, planning, scan execution, and using AWS Glue Data Catalog column statistics. To achieve the best performance for data lake queries with Redshift, you can use AWS Glue Data Catalog column statistics to collect statistics on Data Lake tables. For Amazon Redshift Serverless instances, you\u0026rsquo;ll see scan performance improvements due to increased parallel processing of S3 files, and this happens automatically based on RPUs used.\nIn this post, we highlight the performance improvements observed using industry-standard TPC-DS benchmarks. The total execution time of the TPC-DS 3 TB benchmark improved by 3x. Some queries in the benchmark achieved speeds up to 12x faster.\nPerformance improvements Over the past year, there have been many performance optimizations made to improve the performance of data lake queries, including:\nUsing AWS Glue Data Catalog column statistics and tuning the Redshift optimizer to improve query plan quality. Using bloom filters for partition columns. Improving scan efficiency for Amazon Redshift Serverless instances through increased parallel processing of files. Applying novel query rewrite rules to merge similar scans. Faster metadata retrieval from AWS Glue Data Catalog. To understand the performance gains, we tested performance using the industry-standard TPC-DS benchmark with 3 TB datasets and queries, representing different customer use cases. Performance was tested on a Redshift serverless data warehouse with 128 RPU. In the test, the dataset was stored on Amazon S3 in Parquet format, and AWS Glue Data Catalog was used to manage external databases and tables. Fact tables were partitioned by date column, with each table containing approximately 2,000 partitions. All tables had the row count table property, numRows, set according to spectrum query performance guidelines.\nWe performed a baseline run on last year\u0026rsquo;s Redshift patch version (patch 172). Then, we ran all TPC-DS queries on the latest patch version (patch 180), including all performance optimizations added over the past year. Next, we used AWS Glue Data Catalog column statistics to compute statistics for all tables and measured the performance improvement when column statistics are available.\nAnalysis showed that the TPC-DS 3TB Parquet benchmark achieved significant performance gains thanks to these optimizations. Specifically, partitioned Parquet with the latest optimizations achieved 2x faster runtime compared to the previous implementation. Enabling AWS Glue Data Catalog column statistics further improved performance by 3x compared to last year. The graph below illustrates runtime improvements for the entire benchmark (all TPC-DS queries) over the past year, including the additional boost from using AWS Glue Data Catalog column statistics.\nFigure 1: Total execution time improvements for TPC-DS 3T workload\nThe graph below presents the top queries from the TPC-DS benchmark with the largest performance improvements over the past year, with and without AWS Glue Data Catalog column statistics. You can see that performance improves significantly when statistics are available on AWS Glue Data Catalog (for details on how to collect statistics for Data Lake tables, please refer to optimizing query performance using AWS Glue Data Catalog column statistics). Specifically, multi-join queries will benefit most from AWS Glue Data Catalog column statistics, because the optimizer uses statistics to choose the appropriate join order and distribution strategy.\nFigure 2: Speed-up in TPC-DS queries\nLet\u0026rsquo;s discuss some optimizations that have contributed to improved query performance.\nOptimization with table-level statistics Amazon Redshift is designed to handle large-scale data challenges with superior speed and cost-efficiency. The massively parallel processing (MPP) query engine, AI-powered query optimizer, auto-scaling capabilities, and other advanced features help Redshift excel at searching, aggregating, and transforming petabytes of data.\nHowever, even the most powerful systems can experience performance degradation if they encounter anti-patterns such as extremely inaccurate table statistics, for example, row count metadata.\nWithout this critical metadata, Redshift\u0026rsquo;s query optimizer can be limited in the number of possible optimizations, especially those related to data distribution during query execution. This can significantly impact overall query performance.\nTo illustrate, consider a simple query involving an inner join between a large table with billions of rows and a small table with only a few hundred thousand rows.\nselect small_table.sellerid, sum(large_table.qtysold) from large_table, small_table where large_table.salesid = small_table.listid and small_table.listtime \u0026gt; \u0026#39;2023-12-01\u0026#39; and large_table.saletime \u0026gt; \u0026#39;2023-12-01\u0026#39; group by 1 order by 1 If executed as-is, with the large table on the right-hand side of the join, the query will lead to sub-optimal performance. This is because the large table will need to be distributed (broadcast) to all Redshift compute nodes to perform the inner join with the small table, as illustrated in the diagram below.\nFigure 3: Inaccurate table statistics lead to limited optimizations and large amounts of data being broadcast between compute nodes for a simple inner join\nNow, consider a scenario where table statistics, such as row count, are accurate. This allows the Amazon Redshift query optimizer to make more informed decisions, such as determining the optimal join order. In this case, the optimizer will immediately rewrite the query so the large table is on the left-hand side of the inner join, making the small table the one broadcast to Redshift compute nodes, as illustrated in the diagram below.\nFigure 4: Accurate table statistics lead to a high degree of optimizations and very little data being broadcast between compute nodes for a simple inner join\nFortunately, Amazon Redshift automatically maintains accurate table statistics for local tables by running ANALYZE in the background. However, for external tables (data lake tables), using AWS Glue Data Catalog column statistics is recommended when working with Amazon Redshift, as we will discuss in the next section. For more general information about optimizing queries in Amazon Redshift, please refer to the documentation on factors affecting query performance, data redistribution, and Amazon Redshift best practices for designing queries.\nPerformance improvements with AWS Glue Data Catalog column statistics AWS Glue Data Catalog has a feature to compute column-level statistics for Amazon S3-backed external tables. AWS Glue Data Catalog can compute column-level statistics such as NDV, Number of Nulls, Min/Max, and Avg. column width for columns without requiring additional data pipelines.\nAmazon Redshift\u0026rsquo;s cost-based optimizer uses these statistics to generate better quality query plans. In addition to using statistics, we also made improvements in cardinality estimations and cost tuning to obtain high-quality query plans, thereby improving query performance.\nWith the TPC-DS 3TB dataset, total query execution time improved by 40% when using AWS Glue Data Catalog column statistics. Some individual TPC-DS queries improved up to 5x in execution time. Some queries with significant impact on execution time include Q85, Q64, Q75, Q78, Q94, Q16, Q04, Q24, and Q11.\nWe\u0026rsquo;ll go through an example where the cost-based optimizer generates a better query plan thanks to statistics and how it improves execution time.\nConsider the simpler version of TPC-DS Q64 below to illustrate query plan differences when statistics are available.\nselect i_product_name product_name ,i_item_sk item_sk ,ad1.ca_street_number b_street_number ,ad1.ca_street_name b_street_name ,ad1.ca_city b_city ,ad1.ca_zip b_zip ,d1.d_year as syear ,count(*) cnt ,sum(ss_wholesale_cost) s1 ,sum(ss_list_price) s2 ,sum(ss_coupon_amt) s3 FROM tpcds_3t_alls3_pp_ext.store_sales ,tpcds_3t_alls3_pp_ext.store_returns ,tpcds_3t_alls3_pp_ext.date_dim d1 ,tpcds_3t_alls3_pp_ext.customer ,tpcds_3t_alls3_pp_ext.customer_address ad1 ,tpcds_3t_alls3_pp_ext.item WHERE ss_sold_date_sk = d1.d_date_sk AND ss_customer_sk = c_customer_sk AND ss_addr_sk = ad1.ca_address_sk and ss_item_sk = i_item_sk and ss_item_sk = sr_item_sk and ss_ticket_number = sr_ticket_number and i_color in (\u0026#39;firebrick\u0026#39;,\u0026#39;papaya\u0026#39;,\u0026#39;orange\u0026#39;,\u0026#39;cream\u0026#39;,\u0026#39;turquoise\u0026#39;,\u0026#39;deep\u0026#39;) and i_current_price between 42 and 42 + 10 and i_current_price between 42 + 1 and 42 + 15 group by i_product_name ,i_item_sk ,ad1.ca_street_number ,ad1.ca_street_name ,ad1.ca_city ,ad1.ca_zip ,d1.d_year Without using Statistics The figure below shows the logical query plan of Q64. You can notice that the cardinality estimation of the joins is inaccurate. With inaccurate cardinalities, the optimizer will generate a sub-optimal query plan, leading to higher execution time.\nFigure 5: Logical query plan of Q64 when not using statistics\nUsing Statistics The figure below shows the logical query plan after using AWS Glue Data Catalog column statistics. Based on the highlighted changes, you can see that the cardinality estimations of JOINs have improved significantly, helping the optimizer choose a better join order and join strategy (broadcast DS_BCAST_INNER vs. distribute DS_DIST_BOTH). The swapping of customer_address and customer tables from inner to outer table and changing join strategies to distribute has a large impact because this reduces data movement between nodes and avoids spilling from hash tables.\nFigure 6: Logical query plan of Q64 after consuming AWS Glue Data Catalog column statistics\nThis change in query plan improved Q64\u0026rsquo;s query execution time from 383s to 81s. With the greater benefits of using AWS Glue Data Catalog column statistics for the optimizer, you should consider collecting stats for your data lake using AWS Glue. If your workload is a JOIN-heavy workload, collecting stats will bring greater improvements to your workload. Please refer to generating AWS Glue Data Catalog column statistics for guidance on how to collect statistics in AWS Glue Data Catalog.\nOptimization by query rewriting We introduced a new query rewrite rule that combines scalar aggregates on the same common expression but using slightly different predicates. This rewrite has led to performance improvements on TPC-DS queries Q09, Q28, and Q88. Let\u0026rsquo;s focus on Q09 as a representative of these queries, presented in the fragment below:\nSELECT CASE WHEN (SELECT COUNT(*) FROM store_sales WHERE ss_quantity BETWEEN 1 AND 20) \u0026gt; 48409437 THEN (SELECT AVG(ss_ext_discount_amt) FROM store_sales WHERE ss_quantity BETWEEN 1 AND 20) ELSE (SELECT AVG(ss_net_profit) FROM store_sales WHERE ss_quantity BETWEEN 1 AND 20) END AS bucket1, \u0026lt;\u0026lt;4 more variations of the CASE expression above\u0026gt;\u0026gt; FROM reason WHERE r_reason_sk = 1 There are a total of 15 scans of the fact table store_sales, each scan returning different aggregates on different subsets of data. The query engine first performs subquery removal and transforms the different expressions in CASE statements into relational subtrees connected through cross products, then they are fused into a single subquery handling all scalar aggregates. The resulting query plan for Q09, described below in SQL for easier understanding, is as follows:\nSELECT CASE WHEN v1 \u0026gt; 48409437 THEN t1 ELSE e1 END, \u0026lt;4 more variations\u0026gt; FROM (SELECT COUNT(CASE WHEN b1 THEN 1 END) AS v1, AVG(CASE WHEN b1 THEN ss_ext_discount_amt END) AS t1, AVG(CASE WHEN b1 THEN ss_net_profit END) AS e1, \u0026lt;4 more variations\u0026gt; FROM reason, (SELECT *, ss_quantity BETWEEN 1 AND 20 AS b1, \u0026lt;4 more variations\u0026gt; FROM store_sales WHERE ss_quantity BETWEEN 1 AND 20 OR \u0026lt;4 more variations\u0026gt;)) WHERE r_reason_sk = 1) Overall, this rewrite rule brings the greatest improvements both in latency (3x to 8x speedup) and bytes read from Amazon S3 (6x to 8x reduction in scanned bytes and, accordingly, cost reduction).\nBloom filter for partition columns Amazon Redshift has been using Bloom filters on data columns of external tables in Amazon S3 to enable early and effective data filtering. Last year, we extended this support to partition columns as well.\nA Bloom filter is a probabilistic, memory-efficient data structure that helps accelerate join queries at scale by filtering rows that don\u0026rsquo;t match the join relation, thereby significantly reducing the amount of data transferred over the network. Amazon Redshift automatically identifies suitable queries to use Bloom filters at query runtime.\nThis optimization has brought performance improvements to TPC-DS queries Q05, Q17, and Q54, with large improvements both in latency (2x to 3x speedup) and bytes read from S3 (9x to 15x reduction in scanned bytes and accordingly cost reduction).\nBelow is a subquery of Q05 illustrating improvements with runtime filter.\nselect s_store_id, sum(sales_price) as sales, sum(profit) as profit, sum(return_amt) as returns, sum(net_loss) as profit_loss from ( select ss_store_sk as store_sk, ss_sold_date_sk as date_sk, ss_ext_sales_price as sales_price, ss_net_profit as profit, cast(0 as decimal(7,2)) as return_amt, cast(0 as decimal(7,2)) as net_loss from tpcds_3t_alls3_pp_ext.store_sales union all select sr_store_sk as store_sk, sr_returned_date_sk as date_sk, cast(0 as decimal(7,2)) as sales_price, cast(0 as decimal(7,2)) as profit, sr_return_amt as return_amt, sr_net_loss as net_loss from tpcds_3t_alls3_pp_ext.store_returns ) salesreturnss, tpcds_3t_alls3_pp_ext.date_dim, tpcds_3t_alls3_pp_ext.store where date_sk = d_date_sk and d_date between cast(\u0026#39;1998-08-13\u0026#39; as date) and (cast(\u0026#39;1998-08-13\u0026#39; as date) + 14) and store_sk = s_store_sk group by s_store_id Without bloom filter support on partition columns The figure below is the logical query plan for the sub-query of Q05. This query appends two large fact tables: store_sales (8B rows) and store_returns (863M rows), then joins with very selective dimension tables date_dim and then with dimension table store. You can observe that the join with the date_dim table has reduced the number of rows from 9B to 93M rows.\nFigure 7: Logical query plan for sub-query of Q05 without bloom filter support on partition columns\nWith bloom filter support on partition columns With support of bloom filter on partition columns, we now create a bloom filter for the d_date_sk column of the date_dim table and push down these bloom filters to the store_sales and store_returns tables.\nThese bloom filters help filter out partitions in both store_sales and store_returns tables because the join occurs on the partition column (the number of partitions processed decreases by 10x).\nFigure 8: Logical query plan for sub-query of Q05 with bloom filter support on partition columns\nOverall, the bloom filter on partition column will reduce the number of partitions processed, leading to reduced S3 listing calls and reduced number of data files that need to be read (reduction in scanned bytes). You can see that we only scan 89M rows from store_sales and 4M rows from store_returns thanks to the bloom filter. This reduction in the number of rows at the JOIN level has helped improve overall query performance by 2x and scanned bytes by 9x.\nConclusion In this post, we presented the new performance optimizations in Amazon Redshift data lake query processing and how AWS Glue Data Catalog statistics help enhance the quality of query plans for data lake queries in Amazon Redshift. These optimizations have significantly improved query performance, with the TPC-DS 3TB benchmark showing a 3x improvement in total execution time. Some individual queries achieved speeds up to 12x faster. We recommend using AWS Glue Data Catalog column statistics, especially for JOIN-heavy workloads, to achieve optimal query performance for your data lake queries.\nIn summary, Amazon Redshift now provides enhanced query performance with optimizations such as: AWS Glue Data Catalog column statistics, bloom filters on partition columns, new query rewrite rules, and faster retrieval of metadata. These optimizations are enabled by default, and Amazon Redshift users will benefit from better query response times for their workloads.\nFor more information, please contact your AWS technical account manager or AWS account solutions architect. They will be ready to provide additional guidance and support.\nAbout the Authors Kalaiselvi Kamaraj Kalaiselvi Kamaraj is a Sr. Software Development Engineer at Amazon. She has been involved in many projects in the Redshift Query Processing team and is currently focusing on performance-related projects for Redshift Data Lake.\nMark Lyons Mark Lyons is a Principal Product Manager in the Amazon Redshift team. He works at the intersection of data lakes and data warehouses. Before joining AWS, Mark held product leadership positions at Dremio and Vertica. He is passionate about data analytics and wants to empower customers to change the world with their data.\nAsser Moustafa Asser Moustafa is a Principal Worldwide Specialist Solutions Architect at AWS, based in Dallas, Texas, USA. He collaborates with customers globally, advising on all aspects of data architectures, migrations, and strategic data visions to help organizations adopt cloud-based solutions, maximize the value of their data assets, modernize legacy infrastructures, and deploy cutting-edge capabilities such as machine learning and advanced analytics. Before joining AWS, Asser held many data and analytics leadership positions, and completed an MBA at New York University and an MS in Computer Science at Columbia University, New York. He is passionate about helping organizations become truly data-driven and harnessing the transformative potential from their data.\nTAGS: Amazon Redshift Spectrum, Analytics, Data Lake, optimization, Performance\n"},{"uri":"https://thienluhoan.github.io/workshop-template/3-blogstranslated/3.3-blog3/","title":"Blog 3","tags":[],"description":"","content":"Modernizing real-time payment orchestration on AWS by Neeraj Kaushik, Subhash Sharma, and Venkat Gomatham | on 01 OCT 2025 in | Amazon API Gateway | Amazon CloudFront | Amazon Elastic Container Service | Amazon Elastic Kubernetes Service | Amazon Managed Streaming for Apache Kafka (Amazon MSK) | Architecture | AWS Lambda | AWS Partner Network | AWS Solutions Implementations | Financial Services | Partner solutions | Public Sector | Public Sector Partners | Regions Permalink\nThe global real-time payment market is experiencing significant growth. According to Fortune Business Insights, the market was valued at $24.91 billion in 2024 and is expected to grow to $284.49 billion by 2032, with a CAGR of 35.4%. Similarly, Grand View Research reports that the global mobile payment market, valued at $88.50 billion in 2024, is expected to grow with a CAGR of 38.0% from 2025 to 2030.\n(Note: Third-party research and statistics are provided for reference purposes only. AWS and IBM do not guarantee the accuracy of this information.)\nThis rapid expansion highlights the urgency for financial institutions to modernize their payment processing infrastructure. Financial institutions often need to handle large transaction volumes with near-zero latency to meet strict Service Level Agreements (SLAs), while supporting increasing mobile payment volumes.\nHowever, traditional payment orchestration systems, often built on monolithic architectures, struggle to meet these requirements due to latency, availability, and scalability challenges. Furthermore, reliance on on-premises infrastructure leads to high costs and limits innovation, highlighting the need for modernization.\nAs sustainability becomes a priority, organizations are moving to cloud-based solutions to optimize infrastructure, reduce carbon footprints, and enhance energy efficiency. This shift not only provides scalability and performance but also aligns with global sustainability goals, ensuring a future for real-time payments.\nIn this post, we discuss a real-time payment orchestration framework. This framework uses event-driven architecture and AWS serverless services to enhance the resiliency, efficiency, and scalability of real-time payments. By decomposing payment processing into distinct business capabilities, financial institutions can improve modularity and flexibility. Implementing tenant-based segregation helps with data isolation and security. Additionally, applying asynchronous communication through Amazon Managed Streaming for Apache Kafka (Amazon MSK) enhances scalability and resilience.\nTraditional real-time payment orchestration Payment orchestration serves as a middleware solution, helping streamline transaction processing across multiple payment methods, gateways, and financial institutions. It orchestrates key business functions such as: Payment authorization, Payment processing, Settlement and clearing, Compliance and risk management, Account management, for both inbound and outbound payment flows.\nThe diagram below illustrates the high-level business capabilities supported by payment orchestrators across different payment flows, including real-time payments, digital disbursements, tax payments, wires, and many others.\nA detailed diagram describing the payment processing system with multiple components. The diagram displays primary payment types at the top (including Realtime Payments, Digital Disbursement, Credit Transfer, and Peer to Peer Payments) flowing down to core processing stages including Payment Acceptance, Execution, Clearing, Reporting, Tracking, Reversals, and Billing.\nMany financial institutions adopt a tenant-based approach organized by geography due to differences in clearing processes, localized regulations, and transaction requirements between AWS Regions. However, if services are not properly separated, teams often continue adding region-specific logic to existing services, gradually increasing monolithic complexity and using the same infrastructure for all payment flows.\nTraditional payment systems process transactions linearly, with each step waiting for the previous one to complete. However, analyzing payment workflows reveals many opportunities for parallel execution:\nSanctions screening and fraud detection – Compliance and fraud checks can run concurrently with initial routing decisions, rather than blocking all subsequent processing steps Payment routing and authorization requests – When basic validations complete, routing and authorization can proceed in parallel, not sequentially Payment execution and ledger updates – Actual payment execution doesn\u0026rsquo;t need to wait for ledger records to be updated—they can occur simultaneously Settlement, reconciliation, and tracking – Post-transaction processes can start independently as soon as the primary transaction completes This parallel approach can significantly improve throughput and reduce latency compared to traditional queue-based systems, where operations form a sequential chain, extending processing time and causing bottlenecks.\nMost legacy payment orchestration systems heavily depend on on-premises virtual machines (VMs), leading to several challenges:\nMulti-Region support for disaster recovery and multi-tenancy leads to large capital expenditure and operational overhead High latency and SLA issues due to sequential message processing and delays between globally separated data centers Limited reusability of payment flows because monolithic architectures require region-specific changes for local clearing mechanisms and regulations, increasing complexity and costs Scalability challenges and high memory consumption due to inefficient resource utilization and executing unnecessary logic in multiple regions Complex cross-border payment routing due to differences in clearing rules, transaction limits, and local regulations, increasing latency and routing errors Integration challenges with different data formats because legacy systems rely on proprietary standards (e.g., ISO 20022, SWIFT MT), complicating data conversion and compliance High deployment complexity for new payment flows because monolithic architectures require many region-specific modifications, slowing time to market Environmental impact and high carbon footprint from on-premises infrastructure consuming significant energy, while cloud-based approaches improve efficiency Solution overview To overcome these challenges, the proposed architecture applies the following design principles to build a future-ready real-time payment orchestration solution:\nPerformance at scale – Process over 1,000 transactions per second (TPS) with stable low latency under various load conditions. High availability – Achieve 99.999% uptime to meet strict financial transaction requirements. Geographic resilience – Support global operations with region-specific compliance while maintaining stable performance. Cost optimization – Reduce total cost of ownership (TCO) through efficient resource utilization and serverless technology. Security and compliance – Support data protection and regulatory compliance requirements across different regions. Operational simplicity – Simplify deployment, monitoring, and maintenance across the entire payment ecosystem. Microservices – Separate payment processing into distinct business capabilities, helping financial institutions increase modularity and flexibility. This microservices-based architecture allows independent scaling and development of critical components.\nThe diagram below illustrates the solution architecture (high-level solution architecture) for real-time payments. Existing channels using synchronous or asynchronous APIs can be adapted to use edge-optimized endpoints, helping reduce latency.\nA detailed architecture diagram of an AWS-based payment orchestration platform using event-driven principles. The platform has reusable components deployed across two regions, with specialized modules for payment initiation, execution, reconciliation, billing, and risk management. This architecture uses pub/sub messaging patterns for inter-component communication and connects with enterprise systems including accounting, compliance, and analytics.\nAn event-driven architecture is used for payment orchestration, managing communication through a pub/sub pattern. This architecture maintains persistent connections, improving the performance of the entire real-time payment processing workflow.\nEvent-driven architecture for real-time payment processing allows multiple payment operations to occur simultaneously through different adapters, as opposed to traditional systems where payment processes are executed sequentially through a single pipeline. Payment events are distributed to specialized payment processor microservices based on their functions (initiation, execution, tracking, settlements), allowing each component to process independently without waiting for other components to complete.\nBecause we\u0026rsquo;re moving from sequential processing to distributed processing, maintaining transaction traceability is extremely important. Payment tracking adapters in the diagram connect to enterprise analytics systems, creating a specialized layer to monitor transactions. The pub/sub model allows attaching correlation IDs to events, helping systems track related events across topics and different processing stages.\nA standardized event schema is the foundation of this architecture, ensuring consistency across regional deployments while allowing customization at the adapter level. This schema defines uniform event structures containing tenant-specific metadata and supports versioning to meet future development requirements. By isolating region-specific variations into the adapter layer, the solution maintains core functionality while connecting with diverse enterprise systems through configuration-driven customization rather than code changes.\nFor most payment processes, especially independent processing steps that can run in parallel, this architecture delivers net performance gains despite topic switching overhead, particularly for complex transactions requiring multiple independent validations or concurrent processing steps.\nImplementation on AWS Cloud The solution uses edge-optimized Amazon API Gateway for channels. An edge-optimized API endpoint routes requests to the nearest Amazon CloudFront Point of Presence (POP), which is useful when your customers are geographically distributed, helping efficient routing within each geographical region, improving global responsiveness by minimizing network round trips and ensuring requests follow the shortest possible path before transitioning from the public internet into the client network.\nThe following diagram illustrates the high-level solution architecture for real-time payments.\nThis comprehensive AWS Payment Orchestration solution implements modern cloud-native architecture principles. Core processing logic is executed as Lambda functions, including workflows such as initiation, execution, reconciliation, billing, tracking, risk management, and settlement. The solution leverages Amazon MSK for reliable event streaming between components, with separate Kafka topics for each processing stage. Data persistence is handled by Amazon DynamoDB, supporting cross-region operations. This architecture demonstrates AWS best practices for financial services, including regional redundancy, serverless computing, managed services, and event-driven design patterns. The system integrates with external banking infrastructure and enterprise systems while maintaining separation of concerns through microservices architecture. The solution has built-in support for compliance monitoring, risk management, and payment tracking through specialized Lambda functions.\nThe solution uses Amazon MSK to implement an event-driven architecture, efficiently handling both inbound and outbound channel traffic through API requests and asynchronous message-based events. Amazon MSK communicates using a high-performance binary protocol between producers, consumers, and brokers, ensuring low latency and high throughput. Real-time payments are logically partitioned for multiple tenants across geographical regions—North America, EMEA, LATAM, and Asia-Pacific.\nEach real-time payment tenant follows an active/active disaster recovery strategy by deploying MSK clusters across multiple AWS Regions, designed to achieve high availability and resilience. Amazon MSK provides both serverless and provisioned cluster options. Technical teams can choose either based on non-functional requirements and team expertise. Amazon MSK automatically manages partition leadership with leaders in primary Regions and followers in secondary Regions. During failover, leaders are re-elected in healthy Regions, helping maintain processing capability during regional incidents. Sticky partitioning uses consistent hashing for deterministic routing, and cooperative rebalancing helps efficient failover. Multi-AZ deployment provides zone redundancy and isolated clusters per Region to ensure data sovereignty compliance through programmatic AWS Identity and Access Management (IAM) and Virtual Private Cloud (VPC) boundaries.\nTo support seamless cross-Region replication and maintain message continuity, Amazon MSK Replicator—a fully managed feature of Amazon MSK—is used to replicate topics and synchronize consumer group offsets between clusters. MSK Replicator simplifies building multi-Region Kafka applications without custom code, open-source tool configuration, or infrastructure management. This service automatically provisions and scales necessary resources, allowing teams to focus on business logic while only paying for data being replicated. In the event of a regional outage or failover, traffic can be automatically redirected to healthy Regions without data loss or service interruption, delivering near-zero Recovery Time Objectives (RTOs) and uninterrupted operation for downstream services such as payment processors and audit trail consumers.\nBeyond regional redundancy, this architecture uses an event-driven architecture to enable parallel and decoupled processing of payment transactions. Events such as transaction initiation, validation, and settlement are published asynchronously and consumed by independent microservices, helping drastically reduce end-to-end latency.\nTo handle events at scale, the architecture can use AWS Lambda, Amazon Elastic Container Service (Amazon ECS), or Amazon Elastic Kubernetes Service (Amazon EKS) depending on non-functional requirements. Automatic scaling responds to Amazon CloudWatch metrics, and exponential backoff retry logic combined with dead-letter queues (DLQs) handles throttling scenarios. Circuit breakers are used to prevent cascade failures when high error rates occur.\nOne of the main benefits of the solution is the reusability of payment flows across different regions. Although each region has its own compliance requirements and settlement rules, the core functionalities of real-time payments (such as payment authorization, payment processing, settlement, and clearing) are largely similar. This allows rapid deployment of payment solutions in new regions without needing to rearchitect the entire system.\nFor example, real-time payment systems in the US and UK can share similar business logic for real-time gross settlement, but differ in clearing and compliance requirements. This solution views regions as bounded contexts in microservices architecture, providing flexibility while ensuring each region can handle its own rules and regulations.\nSustainability AWS continuously innovates in designing, building, and operating infrastructure to move toward net-zero carbon by 2040 and become water positive by 2030. Amazon MSK with AWS Graviton-based instances uses up to 60% less energy than equivalent M5 instances, helping you achieve sustainability goals.\nLambda is inherently sustainable by design. Its serverless model ensures compute resources are only used when needed, significantly reducing idle infrastructure and wasted energy. Instead of maintaining always-on servers for infrequent tasks, Lambda provisions compute power just-in-time, achieving near-zero idle capacity.\nSecurity and compliance in financial services Due to the sensitive nature of payment transactions and financial data, you should apply necessary security controls to meet financial regulations such as AWS PCI DSS and AWS Federal Information Processing Standard (FIPS) 140-3 as needed by your organization.\nThe solution should include multi-layered security controls, continuous monitoring, and automated compliance auditing to meet strict requirements from banking regulators and internal risk teams. For more information, refer to Security Guidance.\nConclusion Modernizing payment orchestration systems using event-driven architecture and AWS serverless technologies marks an important step in meeting the needs of today\u0026rsquo;s rapidly evolving financial services industry. This solution addresses key challenges faced by traditional payment systems while delivering significant benefits in performance, scalability, cost optimization, global resilience, sustainability, and compliance.\nBy leveraging advanced cloud technologies and strong security controls, financial institutions can now build a future-ready platform that adapts to changing business needs while maintaining the highest standards of performance, security, and reliability. As the real-time payments market continues to grow strongly, this modern architecture provides a solution that meets current needs while being ready to support future payment innovations. Organizations looking to modernize their payment infrastructure can use this blueprint to accelerate their digital transformation journey, supporting sustainable, secure, and efficient payment processing at scale in an increasingly competitive global market.\nThe architecture presented here is for reference only. IBM will work closely with you to implement the solution according to industry standards and compliance requirements.\nAdditional resources IBM Consulting on AWS AWS for Financial Services Transforming transactions: Streamlining PCI compliance using AWS serverless architecture Best practices for right-sizing your Apache Kafka clusters to optimize performance and cost How to choose the right Amazon MSK cluster type for you AWS Shared Responsibility Model AWS Federal Information Processing Standard (FIPS) 140-3 Sustainability with AWS Graviton AWS PCI DSS IBM Consulting is an AWS Premier Tier Services Partner, helping customers using AWS harness the power of innovation and drive business transformation. They are recognized as a Global Systems Integrator (GSI) with over 22 competencies, including Financial Services Consulting. For more information, please contact an IBM Representative.\nTAGS: AWS partner, Customer Solutions, IBM\nAbout the Authors Neeraj Kaushik Neeraj Kaushik is an AWS Ambassador and Client-Growth Leader at IBM Financial Services. He is an Open Group Certified Distinguished Architect with two decades of experience in direct customer-facing implementation roles, spanning multiple industries, including travel and transportation, banking, retail, education, healthcare, and anti-human trafficking. As a trusted advisor, he works directly with client executives and architects to identify business strategy and build technology roadmaps. As a practicing AWS Professional Certified Architect and Natural Language Processing Expert, he has led many complex cloud modernization programs and AI initiatives.\nSubhash Sharma Subhash Sharma is a Sr. Partner Solutions Architect at AWS. He has over 25 years of experience implementing distributed, scalable, highly available, and secured software products using microservices, AI/ML, Internet of Things (IoT), and blockchain following a DevSecOps approach. In his spare time, Subhash enjoys spending time with family and friends, hiking, walking on the beach, and watching TV.\nVenkat Gomatham Venkat Gomatham is a Senior Partner Solutions Architect at AWS, providing strategic guidance to partners in their cloud transformation journey. With over 20 years of experience as an IT architect, he drives innovation and digital transformation initiatives, helping organizations modernize IT landscapes through advanced technologies such as generative AI and IoT.\n"},{"uri":"https://thienluhoan.github.io/workshop-template/3-blogstranslated/3.4-blog4/","title":"Blog 4","tags":[],"description":"","content":"Getting Started with Healthcare Data Lakes: Using Microservices Data lakes can help hospitals and healthcare facilities turn data into business insights, maintain business continuity, and protect patient privacy. A data lake is a centralized, managed, and secure repository to store all your data, both in its raw and processed forms for analysis. Data lakes allow you to break down data silos and combine different types of analytics to gain insights and make better business decisions.\nThis blog post is part of a larger series on getting started with setting up a healthcare data lake. In my final post of the series, “Getting Started with Healthcare Data Lakes: Diving into Amazon Cognito”, I focused on the specifics of using Amazon Cognito and Attribute Based Access Control (ABAC) to authenticate and authorize users in the healthcare data lake solution. In this blog, I detail how the solution evolved at a foundational level, including the design decisions I made and the additional features used. You can access the code samples for the solution in this Git repo for reference.\nArchitecture Guidance The main change since the last presentation of the overall architecture is the decomposition of a single service into a set of smaller services to improve maintainability and flexibility. Integrating a large volume of diverse healthcare data often requires specialized connectors for each format; by keeping them encapsulated separately as microservices, we can add, remove, and modify each connector without affecting the others. The microservices are loosely coupled via publish/subscribe messaging centered in what I call the “pub/sub hub.”\nThis solution represents what I would consider another reasonable sprint iteration from my last post. The scope is still limited to the ingestion and basic parsing of HL7v2 messages formatted in Encoding Rules 7 (ER7) through a REST interface.\nThe solution architecture is now as follows:\nFigure 1. Overall architecture; colored boxes represent distinct services.\nWhile the term microservices has some inherent ambiguity, certain traits are common:\nSmall, autonomous, loosely coupled Reusable, communicating through well-defined interfaces Specialized to do one thing well Often implemented in an event-driven architecture When determining where to draw boundaries between microservices, consider:\nIntrinsic: technology used, performance, reliability, scalability Extrinsic: dependent functionality, rate of change, reusability Human: team ownership, managing cognitive load Technology Choices and Communication Scope Communication scope Technologies / patterns to consider Within a single microservice Amazon Simple Queue Service (Amazon SQS), AWS Step Functions Between microservices in a single service AWS CloudFormation cross-stack references, Amazon Simple Notification Service (Amazon SNS) Between services Amazon EventBridge, AWS Cloud Map, Amazon API Gateway The Pub/Sub Hub Using a hub-and-spoke architecture (or message broker) works well with a small number of tightly related microservices.\nEach microservice depends only on the hub Inter-microservice connections are limited to the contents of the published message Reduces the number of synchronous calls since pub/sub is a one-way asynchronous push Drawback: coordination and monitoring are needed to avoid microservices processing the wrong message.\nCore Microservice Provides foundational data and communication layer, including:\nAmazon S3 bucket for data Amazon DynamoDB for data catalog AWS Lambda to write messages into the data lake and catalog Amazon SNS topic as the hub Amazon S3 bucket for artifacts such as Lambda code Only allow indirect write access to the data lake through a Lambda function → ensures consistency.\nFront Door Microservice Provides an API Gateway for external REST interaction Authentication \u0026amp; authorization based on OIDC via Amazon Cognito Self-managed deduplication mechanism using DynamoDB instead of SNS FIFO because: SNS deduplication TTL is only 5 minutes SNS FIFO requires SQS FIFO Ability to proactively notify the sender that the message is a duplicate Staging ER7 Microservice Lambda “trigger” subscribed to the pub/sub hub, filtering messages by attribute Step Functions Express Workflow to convert ER7 → JSON Two Lambdas: Fix ER7 formatting (newline, carriage return) Parsing logic Result or error is pushed back into the pub/sub hub New Features in the Solution 1. AWS CloudFormation Cross-Stack References Example outputs in the core microservice:\nOutputs: Bucket: Value: !Ref Bucket Export: Name: !Sub ${AWS::StackName}-Bucket ArtifactBucket: Value: !Ref ArtifactBucket Export: Name: !Sub ${AWS::StackName}-ArtifactBucket Topic: Value: !Ref Topic Export: Name: !Sub ${AWS::StackName}-Topic Catalog: Value: !Ref Catalog Export: Name: !Sub ${AWS::StackName}-Catalog CatalogArn: Value: !GetAtt Catalog.Arn Export: Name: !Sub ${AWS::StackName}-CatalogArn "},{"uri":"https://thienluhoan.github.io/workshop-template/3-blogstranslated/3.5-blog5/","title":"Blog 5","tags":[],"description":"","content":"Getting Started with Healthcare Data Lakes: Using Microservices Data lakes can help hospitals and healthcare facilities turn data into business insights, maintain business continuity, and protect patient privacy. A data lake is a centralized, managed, and secure repository to store all your data, both in its raw and processed forms for analysis. Data lakes allow you to break down data silos and combine different types of analytics to gain insights and make better business decisions.\nThis blog post is part of a larger series on getting started with setting up a healthcare data lake. In my final post of the series, “Getting Started with Healthcare Data Lakes: Diving into Amazon Cognito”, I focused on the specifics of using Amazon Cognito and Attribute Based Access Control (ABAC) to authenticate and authorize users in the healthcare data lake solution. In this blog, I detail how the solution evolved at a foundational level, including the design decisions I made and the additional features used. You can access the code samples for the solution in this Git repo for reference.\nArchitecture Guidance The main change since the last presentation of the overall architecture is the decomposition of a single service into a set of smaller services to improve maintainability and flexibility. Integrating a large volume of diverse healthcare data often requires specialized connectors for each format; by keeping them encapsulated separately as microservices, we can add, remove, and modify each connector without affecting the others. The microservices are loosely coupled via publish/subscribe messaging centered in what I call the “pub/sub hub.”\nThis solution represents what I would consider another reasonable sprint iteration from my last post. The scope is still limited to the ingestion and basic parsing of HL7v2 messages formatted in Encoding Rules 7 (ER7) through a REST interface.\nThe solution architecture is now as follows:\nFigure 1. Overall architecture; colored boxes represent distinct services.\nWhile the term microservices has some inherent ambiguity, certain traits are common:\nSmall, autonomous, loosely coupled Reusable, communicating through well-defined interfaces Specialized to do one thing well Often implemented in an event-driven architecture When determining where to draw boundaries between microservices, consider:\nIntrinsic: technology used, performance, reliability, scalability Extrinsic: dependent functionality, rate of change, reusability Human: team ownership, managing cognitive load Technology Choices and Communication Scope Communication scope Technologies / patterns to consider Within a single microservice Amazon Simple Queue Service (Amazon SQS), AWS Step Functions Between microservices in a single service AWS CloudFormation cross-stack references, Amazon Simple Notification Service (Amazon SNS) Between services Amazon EventBridge, AWS Cloud Map, Amazon API Gateway The Pub/Sub Hub Using a hub-and-spoke architecture (or message broker) works well with a small number of tightly related microservices.\nEach microservice depends only on the hub Inter-microservice connections are limited to the contents of the published message Reduces the number of synchronous calls since pub/sub is a one-way asynchronous push Drawback: coordination and monitoring are needed to avoid microservices processing the wrong message.\nCore Microservice Provides foundational data and communication layer, including:\nAmazon S3 bucket for data Amazon DynamoDB for data catalog AWS Lambda to write messages into the data lake and catalog Amazon SNS topic as the hub Amazon S3 bucket for artifacts such as Lambda code Only allow indirect write access to the data lake through a Lambda function → ensures consistency.\nFront Door Microservice Provides an API Gateway for external REST interaction Authentication \u0026amp; authorization based on OIDC via Amazon Cognito Self-managed deduplication mechanism using DynamoDB instead of SNS FIFO because: SNS deduplication TTL is only 5 minutes SNS FIFO requires SQS FIFO Ability to proactively notify the sender that the message is a duplicate Staging ER7 Microservice Lambda “trigger” subscribed to the pub/sub hub, filtering messages by attribute Step Functions Express Workflow to convert ER7 → JSON Two Lambdas: Fix ER7 formatting (newline, carriage return) Parsing logic Result or error is pushed back into the pub/sub hub New Features in the Solution 1. AWS CloudFormation Cross-Stack References Example outputs in the core microservice:\nOutputs: Bucket: Value: !Ref Bucket Export: Name: !Sub ${AWS::StackName}-Bucket ArtifactBucket: Value: !Ref ArtifactBucket Export: Name: !Sub ${AWS::StackName}-ArtifactBucket Topic: Value: !Ref Topic Export: Name: !Sub ${AWS::StackName}-Topic Catalog: Value: !Ref Catalog Export: Name: !Sub ${AWS::StackName}-Catalog CatalogArn: Value: !GetAtt Catalog.Arn Export: Name: !Sub ${AWS::StackName}-CatalogArn "},{"uri":"https://thienluhoan.github.io/workshop-template/3-blogstranslated/3.6-blog6/","title":"Blog 6","tags":[],"description":"","content":"Getting Started with Healthcare Data Lakes: Using Microservices Data lakes can help hospitals and healthcare facilities turn data into business insights, maintain business continuity, and protect patient privacy. A data lake is a centralized, managed, and secure repository to store all your data, both in its raw and processed forms for analysis. Data lakes allow you to break down data silos and combine different types of analytics to gain insights and make better business decisions.\nThis blog post is part of a larger series on getting started with setting up a healthcare data lake. In my final post of the series, “Getting Started with Healthcare Data Lakes: Diving into Amazon Cognito”, I focused on the specifics of using Amazon Cognito and Attribute Based Access Control (ABAC) to authenticate and authorize users in the healthcare data lake solution. In this blog, I detail how the solution evolved at a foundational level, including the design decisions I made and the additional features used. You can access the code samples for the solution in this Git repo for reference.\nArchitecture Guidance The main change since the last presentation of the overall architecture is the decomposition of a single service into a set of smaller services to improve maintainability and flexibility. Integrating a large volume of diverse healthcare data often requires specialized connectors for each format; by keeping them encapsulated separately as microservices, we can add, remove, and modify each connector without affecting the others. The microservices are loosely coupled via publish/subscribe messaging centered in what I call the “pub/sub hub.”\nThis solution represents what I would consider another reasonable sprint iteration from my last post. The scope is still limited to the ingestion and basic parsing of HL7v2 messages formatted in Encoding Rules 7 (ER7) through a REST interface.\nThe solution architecture is now as follows:\nFigure 1. Overall architecture; colored boxes represent distinct services.\nWhile the term microservices has some inherent ambiguity, certain traits are common:\nSmall, autonomous, loosely coupled Reusable, communicating through well-defined interfaces Specialized to do one thing well Often implemented in an event-driven architecture When determining where to draw boundaries between microservices, consider:\nIntrinsic: technology used, performance, reliability, scalability Extrinsic: dependent functionality, rate of change, reusability Human: team ownership, managing cognitive load Technology Choices and Communication Scope Communication scope Technologies / patterns to consider Within a single microservice Amazon Simple Queue Service (Amazon SQS), AWS Step Functions Between microservices in a single service AWS CloudFormation cross-stack references, Amazon Simple Notification Service (Amazon SNS) Between services Amazon EventBridge, AWS Cloud Map, Amazon API Gateway The Pub/Sub Hub Using a hub-and-spoke architecture (or message broker) works well with a small number of tightly related microservices.\nEach microservice depends only on the hub Inter-microservice connections are limited to the contents of the published message Reduces the number of synchronous calls since pub/sub is a one-way asynchronous push Drawback: coordination and monitoring are needed to avoid microservices processing the wrong message.\nCore Microservice Provides foundational data and communication layer, including:\nAmazon S3 bucket for data Amazon DynamoDB for data catalog AWS Lambda to write messages into the data lake and catalog Amazon SNS topic as the hub Amazon S3 bucket for artifacts such as Lambda code Only allow indirect write access to the data lake through a Lambda function → ensures consistency.\nFront Door Microservice Provides an API Gateway for external REST interaction Authentication \u0026amp; authorization based on OIDC via Amazon Cognito Self-managed deduplication mechanism using DynamoDB instead of SNS FIFO because: SNS deduplication TTL is only 5 minutes SNS FIFO requires SQS FIFO Ability to proactively notify the sender that the message is a duplicate Staging ER7 Microservice Lambda “trigger” subscribed to the pub/sub hub, filtering messages by attribute Step Functions Express Workflow to convert ER7 → JSON Two Lambdas: Fix ER7 formatting (newline, carriage return) Parsing logic Result or error is pushed back into the pub/sub hub New Features in the Solution 1. AWS CloudFormation Cross-Stack References Example outputs in the core microservice:\nOutputs: Bucket: Value: !Ref Bucket Export: Name: !Sub ${AWS::StackName}-Bucket ArtifactBucket: Value: !Ref ArtifactBucket Export: Name: !Sub ${AWS::StackName}-ArtifactBucket Topic: Value: !Ref Topic Export: Name: !Sub ${AWS::StackName}-Topic Catalog: Value: !Ref Catalog Export: Name: !Sub ${AWS::StackName}-Catalog CatalogArn: Value: !GetAtt Catalog.Arn Export: Name: !Sub ${AWS::StackName}-CatalogArn "},{"uri":"https://thienluhoan.github.io/workshop-template/5-workshop/5.3-s3-vpc/5.3.1-create-gwe/","title":"Create VPC &amp; Subnets","tags":[],"description":"","content":" Open Amazon VPC console (Note: Choose region suitable for needs, here the group uses Region ap-southeast-1) Select Create VPC Configuration: Name: HRM-VPC IPv4 CIDR: 10.0.0.0/16 Create Subnets (Split 2 AZs to ensure High Availability): Public Subnets (2): 10.0.1.0/24 \u0026amp; 10.0.2.0/24 (Used for Load Balancer \u0026amp; NAT) Private Subnets (2): 10.0.3.0/24 \u0026amp; 10.0.4.0/24 (Used for App, DB, Redis) Click Create VPC and wait for state to change to Available is successful "},{"uri":"https://thienluhoan.github.io/workshop-template/4-eventparticipated/4.1-event1/","title":"Event 1","tags":[],"description":"","content":"Summary Report: “GenAI-powered App-DB Modernization workshop” Event Objectives Share best practices in modern application design Introduce Domain-Driven Design (DDD) and event-driven architecture Provide guidance on selecting the right compute services Present AI tools to support the development lifecycle Speakers Jignesh Shah – Director, Open Source Databases Erica Liu – Sr. GTM Specialist, AppMod Fabrianne Effendi – Assc. Specialist SA, Serverless Amazon Web Services Key Highlights Identifying the drawbacks of legacy application architecture Long product release cycles → Lost revenue/missed opportunities Inefficient operations → Reduced productivity, higher costs Non-compliance with security regulations → Security breaches, loss of reputation Transitioning to modern application architecture – Microservices Migrating to a modular system — each function is an independent service communicating via events, built on three core pillars:\nQueue Management: Handle asynchronous tasks Caching Strategy: Optimize performance Message Handling: Flexible inter-service communication Domain-Driven Design (DDD) Four-step method: Identify domain events → arrange timeline → identify actors → define bounded contexts Bookstore case study: Demonstrates real-world DDD application Context mapping: 7 patterns for integrating bounded contexts Event-Driven Architecture 3 integration patterns: Publish/Subscribe, Point-to-point, Streaming Benefits: Loose coupling, scalability, resilience Sync vs async comparison: Understanding the trade-offs Compute Evolution Shared Responsibility Model: EC2 → ECS → Fargate → Lambda Serverless benefits: No server management, auto-scaling, pay-for-value Functions vs Containers: Criteria for appropriate choice Amazon Q Developer SDLC automation: From planning to maintenance Code transformation: Java upgrade, .NET modernization AWS Transform agents: VMware, Mainframe, .NET migration Key Takeaways Design Mindset Business-first approach: Always start from the business domain, not the technology Ubiquitous language: Importance of a shared vocabulary between business and tech teams Bounded contexts: Identifying and managing complexity in large systems Technical Architecture Event storming technique: Practical method for modeling business processes Use event-driven communication instead of synchronous calls Integration patterns: When to use sync, async, pub/sub, streaming Compute spectrum: Criteria for choosing between VM, containers, and serverless Modernization Strategy Phased approach: No rushing — follow a clear roadmap 7Rs framework: Multiple modernization paths depending on the application ROI measurement: Cost reduction + business agility Applying to Work Apply DDD to current projects: Event storming sessions with business teams Refactor microservices: Use bounded contexts to define service boundaries Implement event-driven patterns: Replace some sync calls with async messaging Adopt serverless: Pilot AWS Lambda for suitable use cases Try Amazon Q Developer: Integrate into the dev workflow to boost productivity Event Experience Attending the “GenAI-powered App-DB Modernization” workshop was extremely valuable, giving me a comprehensive view of modernizing applications and databases using advanced methods and tools. Key experiences included:\nLearning from highly skilled speakers Experts from AWS and major tech organizations shared best practices in modern application design. Through real-world case studies, I gained a deeper understanding of applying DDD and Event-Driven Architecture to large projects. Hands-on technical exposure Participating in event storming sessions helped me visualize how to model business processes into domain events. Learned how to split microservices and define bounded contexts to manage large-system complexity. Understood trade-offs between synchronous and asynchronous communication and integration patterns like pub/sub, point-to-point, streaming. Leveraging modern tools Explored Amazon Q Developer, an AI tool for SDLC support from planning to maintenance. Learned to automate code transformation and pilot serverless with AWS Lambda to improve productivity. Networking and discussions The workshop offered opportunities to exchange ideas with experts, peers, and business teams, enhancing the ubiquitous language between business and tech. Real-world examples reinforced the importance of the business-first approach rather than focusing solely on technology. Lessons learned Applying DDD and event-driven patterns reduces coupling while improving scalability and resilience. Modernization requires a phased approach with ROI measurement; rushing the process can be risky. AI tools like Amazon Q Developer can significantly boost productivity when integrated into the current workflow. Some event photos Add your event photos here\nOverall, the event not only provided technical knowledge but also helped me reshape my thinking about application design, system modernization, and cross-team collaboration.\n"},{"uri":"https://thienluhoan.github.io/workshop-template/5-workshop/5.1-workshop-overview/","title":"Introduction","tags":[],"description":"","content":"Introduction HumanResource is a Human Resource Management System built on the .NET Core platform, applying a modern 3-Tier Architecture.\nThe goal of this Workshop is to re-platform the application from an On-premise environment to AWS Cloud infrastructure (Cloud Native Migration) while meeting the key principles of the AWS Well-Architected Framework: Security, Reliability, Performance Efficiency, and Cost Optimization.\nWorkshop overview Solution Architecture:\nCompute: Use AWS Elastic Beanstalk (Docker platform) to deploy the HRM application, simplify infrastructure management, and support Auto Scaling as employee data or system load increases.\nDatabase: Utilize Amazon RDS for SQL Server in a Private Subnet to securely store HR information such as employee profiles, contracts, attendance records, and payroll data.\nCaching: Amazon ElastiCache (Redis) is used to store user sessions and cache frequently accessed HR data (employee lists, department structures), improving system response time.\nNetwork \u0026amp; Security:\nVPC: Designed with a Public/Private Subnet model combined with a NAT Gateway, ensuring a secure and isolated HRM environment.\nApplication Layer Security: Implement AWS WAF combined with Amazon CloudFront to protect against web attacks and accelerate content delivery for employees across different regions.\nStorage:\nAmazon S3 is used to store HR documents such as employment contracts, employee records, and training materials with high durability. DevOps:\nA fully automated CI/CD pipeline using AWS CodePipeline and CodeBuild, enabling rapid deployment of HRM features such as payroll processing, attendance management, and leave approval workflows. Monitoring:\nAmazon CloudWatch monitors system health (CPU, Network) and sends alerts to ensure the HRM system remains stable and highly available. "},{"uri":"https://thienluhoan.github.io/workshop-template/5-workshop/5.4-s3-onprem/5.4.1-prepare/","title":"Set up Security Groups","tags":[],"description":"","content":" Access EC2 \u0026gt; Security Groups \u0026gt; Create security group Group 1: Web Server (sg-web-app) Description: Allow HTTP from Internet Inbound Rules: Type: HTTP (80) | Source: 0.0.0.0/0 (Or only from Load Balancer if wanting stricter security) Group 2: Database (sg-db-sql) Description: Allow access only from Web Server Inbound Rules: Type: MSSQL (1433) | Source: Custom \u0026gt; Select ID of sg-web-app Group 3: Redis Cache (sg-redis-cache) Description: Allow access only from Web Server Inbound Rules: Type: Custom TCP (6379) | Source: Custom \u0026gt; Select ID of sg-web-app "},{"uri":"https://thienluhoan.github.io/workshop-template/1-worklog/1.1-week1/","title":"Week 1 Worklog","tags":[],"description":"","content":"Week 1 Objectives: Connect and get acquainted with members of First Cloud Journey (FCJ). Understand basic AWS services, how to use AWS Management Console and AWS CLI. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Get acquainted with FCJ members. - Read and understand the rules and regulations at the internship unit. 11/08/2025 11/08/2025 3 - Set up \u0026amp; manage AWS via Console and CLI. 12/08/2025 12/08/2025 https://cloudjourney.awsstudygroup.com/ 4 - Create AWS Free Tier account. - Learn about AWS Console \u0026amp; AWS CLI. - Practice: + Create AWS account. + Install AWS CLI \u0026amp; configure. + Use AWS CLI basics. 13/08/2025 13/08/2025 https://cloudjourney.awsstudygroup.com/ 5 - Learn basic EC2: + Instance types, AMI, EBS,\u0026hellip; - Methods to remote SSH into EC2. - Learn about Elastic IP. 14/08/2025 15/08/2025 https://cloudjourney.awsstudygroup.com/ 6 - Practice: + Create EC2 instance. + Connect via SSH. + Attach EBS volume. 15/08/2025 15/08/2025 https://cloudjourney.awsstudygroup.com/ Week 1 Achievements: Learned how to draw AWS architecture on draw.io and use standard AWS Architecture Icons.\nSuccessfully created and configured AWS Free Tier account:\nUnderstand the concepts of Root Account and IAM User. Create IAM Group and IAM User, assign administrative permissions. Distinguish between Access Key/Secret Key (CLI) and Console Password (Web login). Familiarized with AWS Management Console:\nKnow how to log in and operate on the web interface. Manage User Group, User, and Permissions in IAM. Understand the importance of setting passwords for Users. Installed and configured AWS CLI v2 on the computer:\nSet up Access Key, Secret Key, default Region. Connect CLI to AWS account using command: aws sts get-caller-identity Used AWS CLI to perform basic operations:\nCheck account information (aws sts get-caller-identity). Get list of regions (aws ec2 describe-regions). View EC2 information (aws ec2 describe-instances). Create and manage key pairs (aws ec2 create-key-pair). Upload files to S3 (aws s3 cp \u0026lt;file\u0026gt; s3://\u0026lt;bucket\u0026gt;). Acquired the ability to combine Console and CLI to manage AWS resources in parallel.\nAdditional learning:\nHow to create domain via Route 53. Introduction to CDN, AWS WAF, and other security services. Manage spending on AWS Billing Dashboard. Learn about AWS Support Plans. "},{"uri":"https://thienluhoan.github.io/workshop-template/1-worklog/","title":"Worklog","tags":[],"description":"","content":"On this page, you will need to introduce your worklog. How did you complete it? How many weeks did you take to complete the program? What did you do in those weeks?\nTypically, and as a standard, a worklog is carried out over about 3 months (throughout the internship period) with weekly contents as follows:\nWeek 1: Getting familiar with AWS and basic AWS services\nWeek 2: Understanding Amazon VPC and network components\nWeek 3: Deep dive into Amazon EC2 and storage services\nWeek 4: Learning Amazon S3 and storage services\nWeek 5: Understanding AWS Security and IAM\nWeek 6: Learning AWS Database Services\nWeek 7: AWS Console and CLI Practice\nWeek 8: AWS VPC and Network Security\nWeek 9: Infrastructure Design for Backend Project\nWeek 10: AWS Services Integration (IAM, S3, SES, Secrets Manager)\nWeek 11: Backend Optimization and Integration Testing\nWeek 12: Project Completion and Documentation\n"},{"uri":"https://thienluhoan.github.io/workshop-template/4-eventparticipated/4.2-event2/","title":"Event 2","tags":[],"description":"","content":"Summary Report: \u0026ldquo;AWS GenAI Builders Club\u0026rdquo; Event Objectives The event aimed to explore AI-driven development lifecycle (AI DLC) and introduce modern AI-powered development tools, focusing on how AI is transforming software development practices and how developers can effectively integrate AI into their workflow.\nSpeakers Toàn Huỳnh - Senior Specialist SA (Solutions Architect)\nTopic: AI-driven Development Lifecycle My Nguyễn - Senior Prototyping Architect\nTopic: Kiro - The AI IDE for Prototype to Production Key Highlights AI-Driven Development Lifecycle (by Toàn Huỳnh) AI Evolution in Software Development:\n2023: AI helping developers write code faster 2024: AI generating larger pieces of code and answering faster 2025: AI completing development tasks end-to-end with human in the loop Key Principles:\nAI DLC is not just a tool - it\u0026rsquo;s a methodology AI should be used for pair-programming, not as a standalone solution Developers must remain the owner and validate all AI-generated code Quality control is the developer\u0026rsquo;s responsibility Challenges with AI at Scale:\nQuality control issues when generating large amounts of code Loss of control and understanding of what AI is doing Risk of having to restart projects if quality is not maintained Token overuse in large projects AI DLC Workflow:\nAI should not make decisions independently - developers make all decisions Use AI to analyze problems, review requirements, and create plans Store results in Markdown files for continuity Break down large projects into smaller units Regular review and validation are essential Use AI for standardized tasks, handle thinking tasks manually Best Practices:\nStart with simple requirements and break them into units Use AI to group important units for end users Implement each unit as a small project Maintain a shared track for backend and frontend collaboration For large projects, consider using Amazon Q instead of simple AI tools Kiro - The AI IDE (by My Nguyễn) Overview:\nKiro is an AI-powered IDE designed for prototype to production development Key differences between Kiro and VSCode were discussed Focus on streamlining the development process from ideation to deployment Key Takeaways AI as a Partner, Not a Replacement:\nView AI as a collaborative partner in problem-solving Maintain human oversight and decision-making authority Quality assurance remains a human responsibility Structured AI Workflow:\nDefine clear roles and responsibilities in prompts Use Markdown files to track progress and maintain context Break down complex projects into manageable units Implement regular review cycles Quality Control:\nNever fully delegate tasks to AI without validation Review AI-generated code and plans regularly Build documentation (Markdown files) to avoid going off track Use AI for standardized tasks, not critical thinking Project Management:\nStart with simple requirements and scope down Use AI to group important units for end users Treat each unit as a separate small project Maintain shared communication channels for team collaboration Tool Selection:\nFor large projects, consider enterprise solutions like Amazon Q Simple AI tools are suitable for smaller, well-defined tasks Choose tools based on project complexity and requirements Applying to Work Implement AI DLC Methodology:\nAdopt a structured approach to AI-assisted development Create Markdown documentation for project planning and tracking Establish clear workflows for AI collaboration Quality Assurance:\nAlways review and validate AI-generated code Implement regular checkpoints in the development process Maintain ownership of all deliverables Project Breakdown:\nBreak large projects into smaller, manageable units Use AI to help identify and group important components Track progress using checkboxes in planning documents Team Collaboration:\nEstablish shared tracks for backend and frontend teams Use AI to facilitate communication and planning Maintain clear documentation for team alignment Tool Evaluation:\nEvaluate AI tools based on project needs Consider enterprise solutions for large-scale projects Use appropriate tools for different development phases Event Experience The event provided valuable insights into the practical application of AI in software development. The speakers emphasized the importance of maintaining human control and oversight while leveraging AI capabilities. The discussion on AI DLC methodology was particularly enlightening, showing how to structure AI collaboration effectively.\nThe presentation on Kiro offered a glimpse into next-generation development tools that integrate AI directly into the IDE, potentially streamlining the development process from prototype to production.\nThe key lesson learned is that AI should enhance developer capabilities rather than replace critical thinking and decision-making processes. Successful AI integration requires careful planning, regular review, and maintaining clear ownership of the development process.\nSome event photos Overall, the event not only provided technical knowledge but also helped me reshape my thinking about AI-driven development lifecycle and how to effectively integrate AI tools into the development workflow while maintaining quality and control.\n"},{"uri":"https://thienluhoan.github.io/workshop-template/5-workshop/5.3-s3-vpc/5.3.2-test-gwe/","title":"Configure Internet &amp; NAT Gateway","tags":[],"description":"","content":"Create Internet Gateway In the VPC dashboard click on Internet gateways Then click on Create internet gateway In the Internet gateway creation section, name it as desired then click on Create internet gateway and wait for it to be created\nAfter the Internet gateway is finished creating go to Actions and click on Attach to VPC to attach it to the VPC created in the previous section Create NAT Gateway Create NAT Gateway placed in Public Subnet 1 Assign Elastic IP to have a static address to the Internet "},{"uri":"https://thienluhoan.github.io/workshop-template/5-workshop/5.4-s3-onprem/5.4.2-create-interface-enpoint/","title":"Initialize Amazon RDS","tags":[],"description":"","content":" Access RDS Console \u0026gt; Subnet groups \u0026gt; Create DB subnet group Name: db-private-group Subnets: Select 2 AZs and select exactly 2 Private Subnets Go to Databases \u0026gt; Create database Engine options: Microsoft SQL Server (Express Edition) Templates: Free tier Settings: Set Master Password (remember for later use) Connectivity: VPC: VPC you created for Web Subnet group: db-private-group Public access: No VPC security group: Select Security group you created for database Click Create database "},{"uri":"https://thienluhoan.github.io/workshop-template/5-workshop/5.2-prerequiste/","title":"Prerequiste","tags":[],"description":"","content":"IAM permissions Attach AdministratorAccess in IAM permission policy to the AWS account for easier workflow Note: Using Administrator privileges is recommended only for the Workshop environment to ensure the deployment process is uninterrupted. In a real Production environment, adhere to the Least Privilege principle for each service { \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: \u0026#34;*\u0026#34;, \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; } ] } Source Code GitHub repository containing Java code and a valid Dockerfile "},{"uri":"https://thienluhoan.github.io/workshop-template/2-proposal/","title":"Proposal","tags":[],"description":"","content":"Enterprise HR Management System Comprehensive Cloud-Native HR Solution for Modern Enterprises 1. Executive Summary The Enterprise HR Management System is an integrated human resources management solution designed for medium-sized enterprises in Vietnam, supporting the management of 100-500 employees. The system automates the entire HR workflow, from employee record management, time and attendance, payroll calculation, to performance evaluation. The platform utilizes AWS ECS on EC2 combined with RDS PostgreSQL, S3 storage, and a CI/CD pipeline (GitHub Actions) to ensure optimal cost under $100/month, high performance, comprehensive security, and detailed RBAC (Role-Based Access Control).\n2. Problem Statement Current Challenges Vietnamese businesses rely on Excel or outdated HR software, leading to time consumption and errors. Manual processes (timekeeping, payroll) are not integrated. Lack of automated approval workflows. Difficulty managing detailed access permissions. Weak, non-real-time reporting. High costs for enterprise solutions like SAP or Workday. Proposed Solution The system leverages an AWS ECS architecture optimized for cost efficiency:\nCompute: ECS Fargate on EC2 (t3.medium Reserved Instance) instead of Lambda to achieve 70% savings. Database: RDS PostgreSQL single-AZ (instead of Multi-AZ) to minimize cost. Authentication: AWS Cognito for SSO + JWT. Storage: S3 for documents, basic CloudFront CDN. CI/CD: GitHub Actions + CodeDeploy for automated deployment. Monitoring: CloudWatch logs and alarms. Security: Route 53, WAF, IAM Roles, VPC with public/private subnets. Key Features Single Sign-On (Google, Microsoft 365). Detailed RBAC (Admin, Manager, Employee, Payroll Officer). Check-in/out with GPS validation. Automated payroll with flexible formula configuration. Approval Workflows (leave, salary adjustment). Mobile App (React Native) for attendance. Real-time reporting dashboard. Comprehensive audit logging. Benefits Saves 70% of manual HR processing time. Reduces data entry errors by 90%. Hosting cost is only $80-95/month (85% cheaper than SAP/Workday). Payback period is 10-14 months. 3. Solution Architecture Here is the cloud architecture diagram for the system:\nAWS Services Utilized AWS Service Primary Function AWS Cognito Authentication, SSO (Google/Microsoft), JWT tokens Amazon RDS PostgreSQL (db.t3.micro, single-AZ) Employee, attendance, payroll data AWS ECS Docker containers for backend API + frontend EC2 t3.medium (Reserved Instance) Host ECS tasks (Cost-optimized Compute) Amazon S3 Document storage (CV, contracts, payslips) CloudFront CDN for static assets, reduced S3 bandwidth Route 53 DNS management AWS WAF API protection IAM Roles Fine-grained access control CloudWatch Logs, monitoring, alarms Secrets Manager API keys, DB credentials GitHub Actions CI/CD pipeline AWS CodeDeploy Automated deployment Component Design Authentication Layer Cognito User Pools with JWT (RS256). Custom authorizer middleware in the API. Optional MFA (SMS/TOTP). API Layer Node.js Express.js service on ECS. RESTful endpoints for 15+ resources. Rate limiting, request validation. Cors configured for web/mobile. Business Logic Employee management (CRUD, contracts, skills). Attendance tracking (check-in/out, GPS validation). Leave management (requests, approvals, balance). Payroll engine (salary calculation, tax, insurance). Performance reviews (KPI tracking). Workflow orchestration (approvals via email). Data Layer RDS PostgreSQL: 12 tables (users, employees, departments, attendance_logs, payroll, leave_requests, approvals, etc.). Indexes on: employee_id, department_id, date ranges. Automated backups daily. Storage Layer S3 buckets for documents (CV, contracts, payslips). S3 Lifecycle: transition to Glacier after 30 days. Signed URLs for secure download. CloudFront distribution for fast delivery. Frontend Next.js 14 (React 18) + TypeScript. Material-UI components. Hosted on CloudFront + S3. Mobile app: React Native (iOS/Android) with offline support. CI/CD Pipeline GitHub Actions workflow: code push → build Docker image → push to ECR → deploy to ECS. Automated testing (Jest unit tests). Staging environment before production. 4. Technical Implementation Phase 1: Planning \u0026amp; Setup (Month 1) Requirements gathering. Database schema design (12 tables, ERD). API specification (OpenAPI/Swagger). AWS account setup, VPC, security groups. GitHub repository initialization. Phase 2: Infrastructure \u0026amp; Auth (Months 1-2) VPC with public/private subnets. RDS PostgreSQL provisioning (single-AZ). Cognito setup (email/phone login, SSO). S3 buckets for documents. IAM roles and policies. CI/CD pipeline (GitHub Actions + CodeDeploy). Phase 3: Core APIs \u0026amp; Mobile (Months 2-3) Employee management APIs. Attendance APIs with GPS validation. Check-in/out mobile app MVP. Leave management APIs. Docker containerization. Phase 4: Payroll \u0026amp; Workflows (Months 3-4) Payroll calculation engine. Payslip generation (PDF). Approval workflows (Lambda-free, using ECS scheduled tasks). Email notifications (SES). Audit logging. Phase 5: Analytics \u0026amp; Advanced (Months 4-5) Dashboard (attendance, payroll stats). Performance review module. Training tracking. Reporting (CSV exports). CloudWatch dashboards. Phase 6: Testing \u0026amp; Launch (Months 5-6) Unit \u0026amp; integration testing. UAT with 30 pilot users. Data migration from old system. Security audit (OWASP Top 10). Performance testing. Production deployment. End-user training. Tech Stack Component Technology/Service Backend Node.js 20.x, Express.js, Prisma, Joi/Zod, JWT Database PostgreSQL 15, Automated backups (7 days) Frontend Next.js 14, React 18, TypeScript, Material-UI v5 Mobile React Native with Expo, Google Maps API Infrastructure as Code Terraform, Docker CI/CD GitHub Actions, AWS CodeDeploy 5. Roadmap \u0026amp; Milestones Month Phase Key Deliverable 1 Planning + Infrastructure AWS setup, database schema, API specs 1-2 Auth \u0026amp; Core Setup Login with SSO, RBAC working 2-3 HR Core Modules Employee management, attendance APIs, mobile app 3-4 Payroll \u0026amp; Workflows Salary calculation, approval workflows 4-5 Analytics \u0026amp; Advanced Dashboards, performance reviews, reporting 5-6 Testing \u0026amp; Launch UAT, data migration, go-live 6+ Post-launch Support, optimization, feature enhancements 6. Budget Estimate Monthly AWS Cost (200 employees, 20,000 API calls/day) Service Estimated Cost EC2 t3.medium (Reserved, 1 year) $20 RDS PostgreSQL db.t3.micro $25 S3 storage (100GB, lifecycle) $2.50 S3 requests $1 CloudFront (50GB transfer) $4 Cognito (50K MAU free) $0 Route 53, CloudWatch, Secrets Manager, Data transfer ~$7 Total AWS/month $60-65 Total Hosting (including GitHub Pro): $64-69/month (~$768-828/year)\nDevelopment Cost (One-Time) Category Estimated Cost Backend development (3 devs, 6 months) $30,000 Frontend development $8,000 Mobile app $5,000 DevOps / Infrastructure $3,000 QA \u0026amp; Testing $4,000 Project management $3,000 Total Development $53,000 ROI Analysis Initial cost: $53,000 + $70 * 6 = $53,420 Annual savings: ~$29,000/year (2 FTE reduction + error reduction) Payback period: ~22 months NPV (3 years, 10% discount): $16,000-20,000 7. Risk Assessment \u0026amp; Mitigation Risk Impact Probability Mitigation Data breach High Medium WAF, VPC, encryption, audit logs, MFA RDS single-AZ downtime High Low Multi-AZ failover after Phase 1, automated backup Budget overrun Medium Low AWS Cost Explorer alerts, reserved instances Timeline delay Medium Medium Agile + 20% buffer, MVP approach User adoption High Medium Training, change management, pilot program Disaster Recovery Strategy RTO \u0026lt; 4 hours (restore from S3 backup) RPO \u0026lt; 1 hour (daily backups) Tested restore procedure monthly Post-launch: upgrade to Multi-AZ RDS 8. Expected Outcomes Technical Improvements 85% of HR processes automated. Real-time dashboard. \u0026lt; 2s API response time. 70% employee mobile app usage. Single source of truth. Business Value HR team workload reduced by 60% (manual tasks). Employee satisfaction increased by 40% (self-service). 100% audit trail for compliance. Payroll accuracy 99.5%. Cost savings $29,000/year. Long-Term Vision 1-2 years of data for AI/ML application. Platform scalability for new branches. Templatized HR modules. Competitive advantage through technology. "},{"uri":"https://thienluhoan.github.io/workshop-template/1-worklog/1.2-week2/","title":"Week 2 Worklog","tags":[],"description":"","content":"Week 2 Objectives: Understand Amazon VPC architecture and how network components work in AWS. Master the concepts of Subnet, Route Table, Security Group, NACL, VPC Endpoint, Peering, and Load Balancer. Know how to choose appropriate connectivity solutions between on-premises and AWS Cloud environments. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Learn concepts \u0026amp; structure of Amazon VPC, account limits, CIDR IPv4/IPv6. - Configure basic VPC via Console. 18/08/2025 18/08/2025 https://cloudjourney.awsstudygroup.com/ 3 - Learn about Subnet, Availability Zone, IP addresses. - Create public/private subnets and assign corresponding route tables. 19/08/2025 19/08/2025 https://cloudjourney.awsstudygroup.com/ 4 - Learn about Route Table, Internet Gateway, NAT Gateway, VPC Endpoint. - Distinguish interface \u0026amp; gateway endpoints. 20/08/2025 20/08/2025 https://cloudjourney.awsstudygroup.com/ 5 - Event: Attend Vietnam Cloud Day 2025 – Track 1: GenAI and Data. - Overview of AI applications in cloud infrastructure. 21/08/2025 21/08/2025 6 - Study Security Group, Network ACL, VPC Flow Logs. - Practice logging \u0026amp; access control between subnets. 22/08/2025 22/08/2025 https://cloudjourney.awsstudygroup.com/ 7 - Learn about VPC Peering, Transit Gateway, VPN, Direct Connect, Elastic Load Balancer (ALB, NLB, CLB, GLB). 23/08/2025 23/08/2025 https://cloudjourney.awsstudygroup.com/ Week 2 Achievements: Understand VPC (Virtual Private Cloud) is a virtual network environment in AWS that helps separate and manage resources by environment (Production/Dev/Test/Staging).\nDefault limit: 5 VPCs per AWS account. Each VPC requires an IPv4 CIDR block (required) and can add IPv6 (optional). Subnet:\nEach subnet is located in a specific Availability Zone. When creating a subnet, must specify CIDR within the VPC\u0026rsquo;s CIDR range. AWS reserves the first 5 IP addresses in each subnet for internal use. Route Table:\nDetermines routing paths in the network. By default, there is one default route table (cannot be deleted) that allows subnets within the VPC to communicate internally. Elastic Network Interface (ENI) \u0026amp; Elastic IP:\nENI is a virtual network card that can be attached to EC2 and moved between instances. Elastic IP is a static public IPv4 address that can be attached to ENI. VPC Endpoint:\nConnects resources within VPC to AWS services without Internet. Two types: Interface Endpoint – uses ENI + private IP. Gateway Endpoint – uses route table (supports S3 and DynamoDB). Internet Gateway \u0026amp; NAT Gateway:\nInternet Gateway: allows EC2 in public subnets to access Internet, managed by AWS, no autoscale configuration needed. NAT Gateway: allows private subnets to access Internet (outbound only). Security Group (SG) and Network ACL (NACL):\nSG is a stateful firewall applied to ENI, only has \u0026ldquo;allow\u0026rdquo; rules. NACL is a stateless firewall applied to subnet, has \u0026ldquo;allow/deny\u0026rdquo; rules and reads from top to bottom. Default: SG blocks inbound, allows outbound; NACL allows all. VPC Flow Logs:\nRecords IP traffic to/from VPC, subnet, or ENI. Stored in CloudWatch Logs or S3, does not record packet content. VPC Peering \u0026amp; Transit Gateway:\nVPC Peering: direct connection between 2 VPCs (1:1), no transitive routing support, CIDR cannot overlap. Transit Gateway: hub center connecting multiple VPCs or on-premises networks. VPN Site-to-Site \u0026amp; Client-to-Site:\nSite-to-Site: connects data center to AWS via Virtual Private Gateway and Customer Gateway. Client-to-Site: allows 1 host to access resources within VPC (VPN client). AWS Direct Connect:\nCreates physical connection from data center to AWS (via Direct Connect Partner in VN). Low latency (~20–30ms), bandwidth can be adjusted. Elastic Load Balancing (ELB):\nDistributes traffic to multiple EC2/Container. Four types: Application Load Balancer (ALB) – Layer 7, supports path-based routing. Network Load Balancer (NLB) – Layer 4, high performance, supports static IP. Classic Load Balancer (CLB) – Layer 4 \u0026amp; 7, legacy, rarely used. Gateway Load Balancer (GLB) – Layer 3, uses GENEVE protocol (port 6081). Supports Sticky Session and Access Logs (stored in S3). Attended Vietnam Cloud Day 2025 - Track 1: GenAI and Data event, which helped better understand how AWS combines artificial intelligence and big data to optimize infrastructure, accelerate data analysis, and enhance customer experience in cloud-native systems.\nUnderstand how to separate environments, secure resources, and route in AWS VPC.\nMaster hybrid connectivity models (VPN, Direct Connect) and AWS network scalability and security capabilities.\n"},{"uri":"https://thienluhoan.github.io/workshop-template/4-eventparticipated/4.3-event3/","title":"Event 3","tags":[],"description":"","content":"Summary Report: \u0026ldquo;AWS Cloud Mastery Series #1\u0026rdquo; Event Objectives The event aimed to explore Generative AI (Gen AI) and its applications, focusing on foundation models, Amazon Bedrock, and how cloud technology can help organizations modernize their platforms while reducing costs compared to building new infrastructure from scratch.\nSpeakers Lâm Tuấn Kiệt - Senior DevOps Engineer, FPT Software Đặng Hoàng Hiếu Nghĩa - AI Engineer, Renova Cloud Đinh Lê Hoàng Anh - Cloud Engineer Trainee, First Cloud AI Journey Key Highlights The Challenge with Legacy Platforms Current platforms are outdated and require modernization Building new platforms from scratch requires high investment costs Solution: Leverage cloud technology and server infrastructure to reduce costs while modernizing Understanding Generative AI (Gen AI) Definition:\nGen AI is the production of text or images based on input prompts It is a foundation model that can handle multiple tasks and is more generalized than traditional AI models Key Characteristics:\nFoundation models are versatile and can be applied to various use cases More generalized approach compared to task-specific AI models Can generate content (text, images) based on prompts Amazon Bedrock Overview:\nAmazon Bedrock is a fully managed service that makes foundation models from leading AI companies accessible via an API It retrieves internal information along with user prompts to generate the most polished output Key Features:\nAccess to multiple foundation models through a single API Retrieval of internal/private information Integration with user prompts Generation of high-quality, well-formatted outputs Key Takeaways Cloud Migration Strategy:\nInstead of investing heavily in new platform infrastructure, organizations can leverage cloud services Cloud technology offers cost-effective solutions for platform modernization Server-based cloud infrastructure provides scalability and flexibility Generative AI Fundamentals:\nGen AI produces content (text/images) based on input prompts Foundation models are more versatile than traditional task-specific models These models can handle multiple tasks with a generalized approach Amazon Bedrock Benefits:\nProvides access to multiple foundation models through unified API Can retrieve and utilize internal/private information Generates polished, professional outputs by combining internal data with user prompts Fully managed service reduces operational overhead Cost Optimization:\nCloud migration can significantly reduce infrastructure costs No need for large upfront investment in new platform development Pay-as-you-go model provides financial flexibility Applying to Work Evaluate Cloud Migration Opportunities:\nAssess current legacy platforms and identify modernization needs Compare costs of building new infrastructure vs. cloud migration Develop migration strategies leveraging cloud services Explore Gen AI Applications:\nIdentify use cases where Gen AI can add value (content generation, automation) Understand foundation models and their capabilities Experiment with prompt engineering for better outputs Leverage Amazon Bedrock:\nEvaluate Amazon Bedrock for accessing foundation models Integrate internal data sources with Bedrock for enhanced outputs Develop workflows that combine internal information with AI-generated content Cost-Benefit Analysis:\nConduct thorough cost analysis before platform modernization decisions Consider cloud solutions as alternatives to building from scratch Factor in scalability and operational costs in decision-making Event Experience The event provided valuable insights into the intersection of cloud technology and Generative AI. The speakers shared practical experiences from their respective roles, offering different perspectives on how organizations can leverage modern technologies.\nLearning from Industry Experts DevOps Perspective: Understanding infrastructure challenges and cloud migration strategies from a senior DevOps engineer AI Engineering View: Deep dive into Gen AI and foundation models from an AI engineer\u0026rsquo;s perspective Trainee Insights: Fresh perspective on cloud engineering and learning journey in the AI space Technical Understanding Gained clarity on what Gen AI is and how it differs from traditional AI approaches Understood the concept of foundation models and their generalized capabilities Learned about Amazon Bedrock as a managed service for accessing AI models Recognized the importance of combining internal data with AI prompts for better outputs Business Value Realized that cloud migration can be a cost-effective alternative to building new platforms Understood how Gen AI can be applied to generate content and automate tasks Learned about the practical applications of Amazon Bedrock in enterprise settings Some event photos Overall, the event successfully bridged the gap between cloud infrastructure and Generative AI, showing how organizations can modernize their platforms cost-effectively while leveraging cutting-edge AI capabilities through services like Amazon Bedrock.\n"},{"uri":"https://thienluhoan.github.io/workshop-template/5-workshop/5.4-s3-onprem/5.4.3-test-endpoint/","title":"Initialize ElastiCache Redis","tags":[],"description":"","content":" Access ElastiCache \u0026gt; Subnet groups \u0026gt; Create subnet group Name: redis-private-group Subnets: Select 2 Private Subnets 2Go to Redis OSS caches \u0026gt; Create cache At Cluster settings screen: Engine: Select Redis OSS Deployment option: Select Node-based cluster Creation method: Select Cluster cache (Configure and create a new cluster) Cluster mode: Select Disabled (Simple mode, 1 Shard) At Location screen: Location: AWS Cloud Multi-AZ: Uncheck (Enable) Note: Disable this feature to save costs for Lab environment Auto-failover: Uncheck (Enable) At Cache settings screen: Engine version: Leave default (Ex: 7.1) Port: 6379 Node type: Select t3 family \u0026gt; Select cache.t3.micro Number of replicas: Enter 0 (We only need 1 primary node, no replica node needed) At Connectivity screen: Network type: IPv4 Subnet groups: Select Choose existing subnet group \u0026gt; Select redis-private-group just created At Advanced settings screen (Important): Encryption at rest: Enable (Default) Encryption in transit: Uncheck (Disable) Reason: Disabling encryption in transit simplifies connection from .NET code in internal VPC environment without configuring complex SSL certificates Selected security groups: Select Manage \u0026gt; select sg-redis-cache (Uncheck default) Scroll to the bottom and click Create 3. Get connection information Initialization process will take about 5-10 minutes\nWhen status changes to Available (Green) Click on Cluster name (webapp or name you set) At Overview tab, find Primary endpoint Copy this connection string (Ex: webapp.xxxx.cache.amazonaws.com) "},{"uri":"https://thienluhoan.github.io/workshop-template/5-workshop/5.3-s3-vpc/","title":"Network Infrastructure Setup","tags":[],"description":"","content":"Overview In this section, we will build the network foundation for the MiniMarket application. A secure network architecture is a prerequisite to protect the application and data\nWe will design a VPC consisting of:\nPublic Subnet: Dedicated to components communicating directly with the Internet (Load Balancer, NAT Gateway). * Private Subnet Dedicated to components requiring security (App Server, Database, Redis) Additionally, we will configure NAT Gateway to allow servers inside the Private Subnet to download updates and Docker Images from the Internet without exposing IP addresses externally "},{"uri":"https://thienluhoan.github.io/workshop-template/3-blogstranslated/","title":"Translated Blogs","tags":[],"description":"","content":"This section will list and introduce the blogs you have translated.\nBlog 1 - How Novo Nordisk, Columbia University and AWS collaborated with the OpenFold AI Consortium to develop OpenFold3 This blog post shares how Novo Nordisk, Columbia University, and AWS collaborated to develop OpenFold3, an advanced protein structure prediction model, using scalable and cost-effective bioinformatics solutions on AWS. The post explores how the team leveraged AWS services and innovative approaches to overcome challenges in training this complex model, including creating a lightweight research environment with Research Collaboration Platform (RCP), optimizing Multiple Sequence Alignment (MSA) generation using AWS Graviton processors, and implementing distributed AI/ML training infrastructure.\nBlog 2 - Accelerate Amazon Redshift Data Lake queries with AWS Glue Data Catalog Column Statistics This blog post highlights performance improvements in Amazon Redshift data lake query processing using AWS Glue Data Catalog column statistics. The post demonstrates how using column statistics, bloom filters on partition columns, new query rewrite rules, and faster metadata retrieval can improve query performance significantly. Using industry-standard TPC-DS benchmarks, the total execution time improved by 3x, with some queries achieving speeds up to 12x faster.\nBlog 3 - Modernizing real-time payment orchestration on AWS This blog post discusses a real-time payment orchestration framework that uses event-driven architecture and AWS serverless services to enhance the resiliency, efficiency, and scalability of real-time payments. The solution addresses challenges faced by traditional payment systems, including high latency, availability issues, and scalability constraints. It demonstrates how decomposing payment processing into distinct business capabilities, implementing tenant-based segregation, and using Amazon MSK for asynchronous communication can improve overall payment processing performance.\nBlog 4 - \u0026hellip; To be updated\nBlog 5 - \u0026hellip; To be updated\nBlog 6 - \u0026hellip; To be updated\n"},{"uri":"https://thienluhoan.github.io/workshop-template/1-worklog/1.3-week3/","title":"Week 3 Worklog","tags":[],"description":"","content":"Week 3 Objectives: Deep dive into Amazon EC2 and related storage services (EBS, EFS, FSx). Understand EC2 operation mechanisms, configuration, and pricing options. Practice creating and managing EC2 Instance, EBS Volume, Snapshot. Understand and apply Auto Scaling, Pricing Options, and Lightsail. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Overview of Amazon EC2: concepts, scalability, comparison with physical servers.\n- Learn about Instance Types, specifications (CPU, Memory, Network, Storage). 25/08/2025 25/08/2025 https://cloudjourney.awsstudygroup.com/ 3 - Study AMI (Amazon Machine Image), EC2 Instance provisioning process.\n- Learn about Hypervisor (KVM, HVM, PV) and selection mechanisms. 26/08/2025 26/08/2025 https://cloudjourney.awsstudygroup.com/ 4 - Learn about Key Pair and login encryption mechanisms (Linux/Windows).\n- Learn about EBS (Elastic Block Store): HDD types, SSD, snapshot, data replication. 27/08/2025 27/08/2025 https://cloudjourney.awsstudygroup.com/ 5 - Study Instance Store: characteristics, advantages/disadvantages, practical applications.\n- Practice backing up EBS using snapshot and recovery. 28/08/2025 28/08/2025 https://cloudjourney.awsstudygroup.com/ 6 - Learn about User Data and Metadata in EC2, how to automate when initializing Instance.\n- Learn about EC2 Auto Scaling, Scaling Policy, Load Balancer integration. 29/08/2025 29/08/2025 https://cloudjourney.awsstudygroup.com/ 7 - Learn about Pricing Options: On-demand, Reserved Instance, Saving Plan, Spot Instance.\n- Study Amazon Lightsail, EFS, FSx, and AWS Application Migration Service (MGN). 30/08/2025 30/08/2025 https://cloudjourney.awsstudygroup.com/ Week 3 Achievements: Understand Amazon EC2 architecture and operation:\nSimilar to traditional virtual machines but with flexible scalability. Can be quickly initialized, supports various workloads such as web, applications, database\u0026hellip; Master EC2 Instance Type, AMI, Hypervisor, Key Pair mechanisms.\nKnow how to create, connect, and backup EC2 Instance through EBS snapshot.\nUnderstand the differences between EBS, Instance Store, EFS, and FSx:\nEBS: block storage directly attached to EC2, operates independently via private network. Instance Store: extremely high speed, does not persist data when instance is stopped. EFS: shared storage for multiple EC2 (Linux). FSx: similar to EFS but supports NTFS and SMB (Windows/Linux). Master EC2 Auto Scaling:\nAutomatically increases/decreases number of Instances. Supports multiple AZs and integrates with Load Balancer. Can combine multiple Pricing Options. Understand 4 main Pricing Options:\nOn-demand: flexible, higher price. Reserved Instance \u0026amp; Saving Plan: savings with long-term commitment. Spot Instance: low price, but may be terminated suddenly. Know how to use Amazon Lightsail for light workloads, test/dev.\nMaster AWS Application Migration Service (MGN) mechanism to replicate physical/virtual servers to EC2.\nUnderstand data replication process, incremental snapshot, and cost optimization through deduplication.\nCompleted documentation and detailed notes on the entire chain of services related to EC2.\n"},{"uri":"https://thienluhoan.github.io/workshop-template/4-eventparticipated/4.4-event4/","title":"Event 4","tags":[],"description":"","content":"Summary Report: \u0026ldquo;AWS Cloud Mastery Series #2\u0026rdquo; Event Objectives The event aimed to explore DevOps mindset and best practices, focusing on understanding what DevOps is, its core principles, measurement strategies, and practical approaches to implementing DevOps culture in organizations.\nSpeakers Trương Quang Tịnh - AWS Community Builder, Platform Engineer at tymeX\nKey Highlights What is DevOps? Definition:\nDevOps is a person who understands and bridges the gap between Development (Dev) and Operations (Ops) There are various tools for managing applications Everything must be automated because code reduces work-related risks Must have systems for evaluation and monitoring to have sources for assessing how applications run Core DevOps Principles 1. Collaboration \u0026amp; Shared Responsibility\nContent:\nDevOps emphasizes that the development (Dev) and operations (Ops) teams must share responsibility for the entire product lifecycle Typical quote: \u0026ldquo;You build it, you run it\u0026rdquo; Why it matters:\nReduces conflict between Dev and Ops Increases initiative in fixing bugs and improving features Reduces deployment and troubleshooting time Real-life example:\nA Dev team deploys a new microservice on AWS Lambda. When there is a bug that increases API latency, the Dev team analyzes the logs from CloudWatch and fixes it without \u0026ldquo;handing the ball\u0026rdquo; to Ops In companies like Amazon, the development team is responsible 24/7 for the service they create 2. Automation Everything\nContent:\nDevOps prioritizes automation as much as possible: CI/CD, test, provisioning, monitoring, rollback, security scanning\u0026hellip; Key principle: \u0026ldquo;Manual work is technical debt\u0026rdquo; Important reasons:\nReduce human error Accelerate deployment Make it easy to repeat the process Enable rapid scaling Real-life examples:\nUse GitHub Actions + Terraform to automatically create AWS infrastructure each time the pipeline runs Each Pull Request automatically runs unit tests, linting, and security scans using tools like SonarQube, Snyk Deploy production applications using ArgoCD or Jenkins with just 1 commit 3. Continuous Learning \u0026amp; Experimentation\nContent:\nDevOps culture encourages learning new things, experimenting, and accepting small failures Key principles: \u0026ldquo;Fail fast, learn faster\u0026rdquo; and \u0026ldquo;Experiment with new tools and practices\u0026rdquo; Why it matters:\nTechnology changes very quickly—DevOps must stay up to date Experimentation helps improve continuous improvement Reduce fear of failure → increase innovation Real-life examples:\nA team switched from Jenkins to GitHub Actions to reduce pipeline runtime from 15 minutes to 5 minutes Host game days on AWS to practice crashing simulated services and test resilience Experiment with Canary Deployment or Blue-Green Deployment to reduce risk when deploying new features 4. Measurement\nContent:\nDevOps requires continuous and real-time measurement Examples: logs, metrics, tracing, SLOs, SLIs Why it\u0026rsquo;s important:\nCan\u0026rsquo;t improve what can\u0026rsquo;t be measured Understand the system health and behavior of the product Reduce time to detect failure (MTTD) and time to recover (MTTR) Practical examples:\nUse Prometheus + Grafana to monitor CPU, latency, error rate Set up AWS CloudWatch Alarms to alert when 5xx error rate exceeds 2% Monitor DORA metrics: Deployment Frequency, Lead Time for Changes, MTTR, Change Failure Rate DevOps Goals and Benefits Monitor Deployment Health:\nMonitor the status of deployments: are there any errors, are there rollbacks, how long does it take to deploy Helps detect deployment errors early Reduces production risks Example: Use AWS CodeDeploy or ArgoCD to track the status of each deployment and automatically rollback if the error rate \u0026gt; 5% Improve Agility:\nAgility = the ability to change quickly DevOps helps businesses release more, faster, and more confidently Release features earlier than competitors Shorten the time from idea to product Example: A startup deploys 20–30 times/day thanks to CI/CD + microservices Ensure System Stability:\nStability = the system runs stably, with little downtime Avoid losing revenue Increase trust with customers Example: Use AWS autoscaling + health checks to ensure the service runs smoothly even when traffic spikes Optimize Customer Experience:\nObserving metrics to improve: response speed, error rate, downtime → better experience Example: Reduce API latency from 500ms to 120ms by optimizing infrastructure and pipeline Justify Technology Investments:\nUse data to prove: DevOps helps reduce errors, speed up releases, and save costs Example: Use DORA metrics to report to leadership that implementing CI/CD reduces time to market by 40% Key Takeaways DevOps is a Mindset, Not Just Tools:\nDevOps is about bridging Dev and Ops, not just using specific tools Understanding the system is more important than knowing tools Automation reduces risks and human errors Four Core Principles:\nCollaboration \u0026amp; Shared Responsibility: \u0026ldquo;You build it, you run it\u0026rdquo; Automation Everything: Manual work is technical debt Continuous Learning \u0026amp; Experimentation: Fail fast, learn faster Measurement: Can\u0026rsquo;t improve what can\u0026rsquo;t be measured Measurement is Critical:\nUse logs, metrics, tracing, SLOs, SLIs Monitor DORA metrics for continuous improvement Real-time monitoring helps reduce MTTD and MTTR DevOps Benefits:\nImprove agility and deployment frequency Ensure system stability and reduce downtime Optimize customer experience Justify technology investments with data Best Practices:\nStart with fundamentals (Linux, networking, Git, cloud basics) Learn by building real projects Document everything for better collaboration Applying to Work Implement Shared Responsibility:\nEncourage Dev teams to take ownership of their services Establish 24/7 responsibility for services created Reduce handoffs between Dev and Ops teams Automate Everything:\nSet up CI/CD pipelines using GitHub Actions or Jenkins Automate infrastructure provisioning with Terraform Implement automated testing, linting, and security scanning Use tools like ArgoCD for automated deployments Establish Measurement Systems:\nSet up monitoring with Prometheus + Grafana or AWS CloudWatch Track DORA metrics (Deployment Frequency, Lead Time, MTTR, Change Failure Rate) Configure alerts for error rates and system health Monitor deployment health and implement automatic rollbacks Foster Continuous Learning:\nOrganize game days to practice resilience testing Experiment with deployment strategies (Canary, Blue-Green) Stay updated with new tools and practices Encourage experimentation and learning from failures Start with Fundamentals:\nBuild strong foundation in Linux, networking, Git, and cloud basics Learn by building real projects (Docker + Nginx, CI/CD pipelines, monitoring) Document everything: steps, errors, and solutions Event Experience The event provided comprehensive insights into DevOps mindset and culture, going beyond just tools and focusing on the fundamental principles that drive successful DevOps implementation.\nUnderstanding DevOps Culture Learned that DevOps is fundamentally about bridging Dev and Ops, not just using specific tools Understood the importance of shared responsibility and collaboration Recognized that automation is essential to reduce risks and human errors Core Principles in Practice Collaboration: Realized how \u0026ldquo;You build it, you run it\u0026rdquo; changes team dynamics and accountability Automation: Understood why \u0026ldquo;Manual work is technical debt\u0026rdquo; and how automation accelerates everything Learning: Appreciated the value of \u0026ldquo;Fail fast, learn faster\u0026rdquo; mindset in fostering innovation Measurement: Recognized that measurement is the foundation for continuous improvement Practical Applications Learned about real-world examples from companies like Amazon Understood how to implement monitoring and measurement systems Gained insights into deployment strategies and automation tools Discovered the importance of DORA metrics for tracking DevOps success Key Insights DevOps is a cultural shift, not just a set of tools Measurement and monitoring are critical for success Automation reduces risks and enables rapid scaling Continuous learning and experimentation drive innovation Some event photos Overall, the event successfully conveyed that DevOps is fundamentally about culture, collaboration, and continuous improvement. The emphasis on measurement, automation, and shared responsibility provides a solid foundation for implementing DevOps practices that deliver real business value.\n"},{"uri":"https://thienluhoan.github.io/workshop-template/5-workshop/5.4-s3-onprem/","title":"Data Layer Deployment","tags":[],"description":"","content":"Overview Data is the most important asset of every system. Therefore we will set up the data layer (Data Layer) for MiniMarket with criteria: Maximum Security and High Performance\nWe will deploy two core services:\nAmazon RDS (Relational Database Service): Use SQL Server to store business data (Products, Orders, Users). Database will be placed in Private Subnet to prevent direct access from the Internet Amazon ElastiCache (Redis): Use Redis as cache memory (In-memory Cache) to store Login Sessions and offload queries for the main Database "},{"uri":"https://thienluhoan.github.io/workshop-template/4-eventparticipated/","title":"Events Participated","tags":[],"description":"","content":" In this section, you should list and describe in detail the events you have participated in during your internship or work experience.\nEach event should be presented in the format Event 1, Event 2, Event 3…, along with the following details:\nEvent name Date and time Location (if applicable) Your role in the event (attendence, event support, speaker, etc.) A brief description of the event’s content and main activities Outcomes or value gained (lessons learned, new skills, contribution to the team/project) This listing helps demonstrate your actual participation as well as the soft skills and experience you have gained from each event. During my internship, I participated in five events. Each one was a memorable experience that provided new, interesting, and useful knowledge, along with gifts and wonderful moments.\nEvent 1 Event Name: GenAI-powered App-DB Modernization workshop\nDate \u0026amp; Time: 09:00, August 13, 2025\nLocation: 26th Floor, Bitexco Tower, 02 Hai Trieu Street, Saigon Ward, Ho Chi Minh City\nRole: Attendence\nEvent 2 Event Name: AWS GenAI Builders Club\nDate \u0026amp; Time: October 3, 2025\nLocation: 26th Floor, Bitexco Tower, 02 Hai Trieu Street, Saigon Ward, Ho Chi Minh City\nRole: Attendence\nEvent 3 Event Name: AWS Cloud Mastery Series #1\nDate \u0026amp; Time: November 15, 2025\nLocation: 26th Floor, Bitexco Tower, 02 Hai Trieu Street, Saigon Ward, Ho Chi Minh City\nRole: Attendence\nEvent 4 Event Name: AWS Cloud Mastery Series #2\nDate \u0026amp; Time: November 17, 2025\nLocation: 26th Floor, Bitexco Tower, 02 Hai Trieu Street, Saigon Ward, Ho Chi Minh City\nRole: Attendence\nEvent 5 Event Name: AWS Cloud Mastery Series #3\nDate \u0026amp; Time: November 29, 2025\nLocation: 26th Floor, Bitexco Tower, 02 Hai Trieu Street, Saigon Ward, Ho Chi Minh City\nRole: Attendence\n"},{"uri":"https://thienluhoan.github.io/workshop-template/1-worklog/1.4-week4/","title":"Week 4 Worklog","tags":[],"description":"","content":"Week 4 Objectives: Deep dive into Amazon Simple Storage Service (S3) and related storage services. Understand object-based storage structure, distinguish from block storage. Master storage classes, lifecycle policies, CORS, and versioning mechanisms. Learn about Amazon Glacier, Snow Family, AWS Storage Gateway, and AWS Backup. Understand RTO/RPO concepts and disaster recovery strategies (Disaster Recovery). Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Introduction to Amazon S3: concepts, characteristics of object storage.\n- Learn about data replication mechanisms, 99.999999% durability and 99.99% availability. 01/09/2025 01/09/2025 https://cloudjourney.awsstudygroup.com/ 3 - Study bucket, object, key, access point.\n- Learn about storage classes: Standard, IA, Intelligent Tiering, One Zone, Glacier, Deep Archive. 02/09/2025 02/09/2025 https://cloudjourney.awsstudygroup.com/ 4 - Configure S3 lifecycle policy to automatically transition storage tiers.\n- Learn about multipart upload and event trigger when uploading/deleting files. 03/09/2025 03/09/2025 https://cloudjourney.awsstudygroup.com/ 5 - Learn about S3 hosting static website, CORS policy, Access Control List (ACL), Bucket Policy, IAM Policy.\n- Practice setting up access permissions. 04/09/2025 04/09/2025 https://cloudjourney.awsstudygroup.com/ 6 - Learn about S3 Versioning, S3 Endpoint, partition \u0026amp; prefix performance optimization.\n- Study Amazon Glacier: data retrieval mechanisms (Expedited, Standard, Bulk). 05/09/2025 05/09/2025 https://cloudjourney.awsstudygroup.com/ 7 - Study Snow Family (Snowball, Snowball Edge, Snowmobile).\n- Learn about AWS Storage Gateway (File, Volume, Tape).\n- Learn about AWS Backup, RTO/RPO, and Disaster Recovery Strategies. 06/09/2025 06/09/2025 https://cloudjourney.awsstudygroup.com/ Week 4 Achievements: Master knowledge about Amazon S3:\nIt is an object storage service, cannot edit parts but must upload entire object. Data is replicated across 3 Availability Zones within the same region. Supports trigger events, multipart upload, versioning, and CORS configuration. Understand storage tiers (Storage Classes):\nS3 Standard: frequently accessed data. S3 Standard-IA / One Zone-IA: infrequently accessed data, lower cost. S3 Intelligent-Tiering: automatically moves data between tiers. S3 Glacier / Deep Archive: long-term storage, slow retrieval, low cost. Practice configuring Lifecycle Policy:\nSet up automatic transition of objects after X days between storage classes. Automatically delete old objects after specified time. Learn about CORS and permission mechanisms:\nCORS (Cross-Origin Resource Sharing) allows web apps to access resources from different domains. S3 ACL: basic access permissions by bucket/object. Bucket Policy / IAM Policy: defines detailed access permissions using JSON policy. Understand Versioning:\nWhen versioning is enabled, deletion or overwriting does not lose old data. Supports recovery of deleted or incorrectly overwritten files. Optimize S3 performance:\nUse random prefix to improve search performance in partitions. Understand S3\u0026rsquo;s partition \u0026amp; key map hash mechanism. Amazon Glacier:\nServes long-term data storage with 3 retrieval levels: Expedited (1–5 minutes) Standard (3–5 hours) Bulk (5–12 hours) Snow Family:\nSnowball / Snowball Edge / Snowmobile used to migrate large-scale data (TB → EB) from on-premise to AWS. Can process, encrypt, and compress data before importing to S3 or Glacier. AWS Storage Gateway:\nCombines on-premise + cloud storage, includes 3 types: File Gateway (NFS/SMB) → writes to S3. Volume Gateway (iSCSI) → stores block data, snapshots to EBS. Tape Gateway (VTL) → stores virtual tapes on S3/Glacier. AWS Backup \u0026amp; Disaster Recovery:\nUnderstand concepts of RTO (Recovery Time Objective) and RPO (Recovery Point Objective). Distinguish 4 DR strategies: Backup \u0026amp; Restore Pilot Light Low Capacity Active-Active Full Capacity Active-Active AWS Backup supports EBS, EC2, RDS, DynamoDB, EFS, Storage Gateway. "},{"uri":"https://thienluhoan.github.io/workshop-template/4-eventparticipated/4.5-event5/","title":"Event 5","tags":[],"description":"","content":"Summary Report: \u0026ldquo;AWS Cloud Mastery Series #3\u0026rdquo; Event Objectives The event aimed to explore AWS Identity and Access Management (IAM) best practices, focusing on security principles, role-based access control, Single Sign-On (SSO), AWS Organizations, credential management, and advanced IAM features for enterprise security.\nSpeakers Huỳnh Hoàng Long Đinh Lê Hoàng Anh Key Highlights Understanding IAM (Identity and Access Management) Definition:\nIAM consists of roles with specific permissions assigned to specific users Increases authentication capabilities for the system Provides fine-grained access control to AWS resources Core Concept:\nRoles define what actions can be performed Users are assigned roles based on their responsibilities Enhances system security through proper access management IAM Best Practices 1. Principle of Least Privilege:\nBest practice: Do not grant an account too many permissions Only grant specific permissions needed for the task Reduces security risks and potential damage from compromised accounts 2. Root Access Key Management:\nDelete root access keys because they have the highest privileges If access keys are lost, it won\u0026rsquo;t cause too much impact Minimizes maximum potential losses Root account should only be used for initial setup and emergency scenarios 3. Avoid Wildcard Permissions:\nAvoid using * (asterisk) which means full permissions for all services Use specific service and action permissions instead Provides better security and auditability 4. Use AWS Login (Temporary Credentials):\nShould use AWS login because it resets at least every 15 minutes and at most every 36 hours Increases security significantly Temporary credentials reduce the risk of long-term exposure Automatically expires, reducing attack surface 5. Multi-Factor Authentication (MFA):\nMFA is an additional authentication step that helps prevent unauthorized access Prevents account login from other locations Adds an extra layer of security beyond passwords Should be enabled for all privileged accounts 6. Regular Password Changes:\nShould regularly change account passwords Follows security best practices for credential management Reduces risk of compromised credentials Single Sign-On (SSO) Definition:\nSSO allows one login to access multiple systems Users authenticate once and gain access to multiple applications When switching to another app, no need to login again Enterprise Benefits:\nBetter management in enterprise environments Multiple roles available, each with specific accounts Centralized access control Simplified user experience AWS Integration:\nWhen activating SSO, simultaneously activates AWS Organizations Enables centralized management of multiple AWS accounts Provides unified access control across the organization AWS Organizations and Service Control Policies (SCP) AWS Organizations:\nEnables management of multiple AWS accounts from a central location Provides consolidated billing and account management Works in conjunction with SSO for enterprise access control Service Control Policies (SCP):\nService control policies that can set maximum permission limits within an account Provides centralized security governance Can restrict what actions can be performed across accounts Helps enforce organizational security policies Permission Boundaries Definition:\nAn advanced IAM service that handles centralized security issues Sets the maximum permissions that an identity-based policy can grant Provides an additional layer of security control Prevents privilege escalation even if policies are misconfigured Use Cases:\nDelegating permissions to developers or teams Ensuring users cannot exceed intended permissions Centralized security management in large organizations Credential Spectrum and Rotation Credential Spectrum:\nUnderstanding different types of credentials and their security implications From long-term access keys to temporary credentials Balancing security with usability Credential Rotation:\nAbout AWS Secrets Manager service Process: Create fake accounts with new passwords, test if they can be used, apply to your account, and finally delete old accounts Automates the rotation of database credentials, API keys, and other secrets Reduces risk of credential compromise Ensures credentials are regularly updated without manual intervention AWS Secrets Manager Benefits:\nAutomatic rotation of secrets Secure storage of credentials Integration with AWS services Audit trail of secret access IAM Access Analyzer Overview:\nTool for analyzing IAM policies and access patterns Identifies resources shared with external entities Helps discover unintended access Provides recommendations for improving security posture Key Features:\nAnalyzes resource-based policies Identifies publicly accessible resources Suggests policy improvements Continuous monitoring of access patterns Key Takeaways IAM Fundamentals:\nIAM roles and users provide granular access control Proper IAM configuration is essential for AWS security Authentication and authorization are separate but related concepts Security Best Practices:\nFollow principle of least privilege Delete root access keys Avoid wildcard permissions Use temporary credentials (AWS login) Enable MFA for all accounts Regularly rotate passwords Enterprise Access Management:\nSSO simplifies access across multiple systems AWS Organizations enables centralized account management SCPs provide organization-wide security governance Permission boundaries prevent privilege escalation Credential Management:\nUse AWS Secrets Manager for credential rotation Prefer temporary credentials over long-term access keys Automate credential rotation where possible Monitor credential usage and access patterns Continuous Security:\nUse IAM Access Analyzer for ongoing security assessment Regularly review and audit IAM policies Monitor for unintended access Stay updated with AWS security best practices Applying to Work Implement IAM Best Practices:\nReview and remove root access keys Enable MFA for all privileged accounts Replace wildcard permissions with specific permissions Use temporary credentials instead of long-term access keys Set Up SSO:\nEvaluate SSO implementation for your organization Configure AWS Organizations if managing multiple accounts Implement role-based access control Centralize user management Implement Permission Boundaries:\nUse permission boundaries for delegated access Set maximum permission limits for teams Prevent privilege escalation Centralize security policies Credential Rotation:\nUse AWS Secrets Manager for database credentials Automate rotation of API keys and secrets Establish rotation schedules Test rotation processes before production Security Monitoring:\nEnable IAM Access Analyzer Regularly review access patterns Identify and remediate unintended access Use CloudTrail for audit logging Regular Security Audits:\nConduct periodic IAM policy reviews Remove unused credentials and roles Update policies based on changing requirements Document access requirements and justifications Event Experience The event provided comprehensive insights into AWS IAM security best practices, covering everything from basic principles to advanced enterprise features. The speakers shared practical experiences and real-world scenarios that highlighted the importance of proper IAM configuration.\nUnderstanding IAM Fundamentals Learned that IAM is about roles with specific permissions assigned to users Understood how proper IAM configuration enhances system authentication Recognized the importance of granular access control Security Best Practices Least Privilege: Realized the importance of granting only necessary permissions Root Access: Understood why root access keys should be deleted Wildcard Avoidance: Learned to avoid * permissions for better security Temporary Credentials: Appreciated the security benefits of AWS login with automatic rotation MFA: Recognized MFA as essential for preventing unauthorized access Password Management: Understood the need for regular password changes Enterprise Features SSO: Learned how Single Sign-On simplifies access across multiple systems AWS Organizations: Understood how it works with SSO for centralized management SCP: Discovered how Service Control Policies enforce organization-wide security Permission Boundaries: Recognized their role in preventing privilege escalation Credential Management Credential Rotation: Learned about AWS Secrets Manager and automated rotation Credential Spectrum: Understood different types of credentials and their security implications Best Practices: Discovered the process of credential rotation and testing Security Tools IAM Access Analyzer: Learned how to use it for continuous security assessment Monitoring: Understood the importance of ongoing access pattern analysis Remediation: Discovered how to identify and fix security issues Some event photos Overall, the event successfully demonstrated that IAM is the foundation of AWS security. The emphasis on best practices, enterprise features, and continuous security monitoring provides a comprehensive approach to managing access and protecting AWS resources effectively.\n"},{"uri":"https://thienluhoan.github.io/workshop-template/5-workshop/5.5-policy/","title":"Application Deployment","tags":[],"description":"","content":"Overview After the network and database infrastructure, the next step is to release the Java Spring Boot application source code (HR Management System) to the Cloud environment. Because I have to manage the work on the EC2 server, I use the PaaS platform - AWS Elastic Beanstalk to automate the process of developing declarations and operations. The goal of this module: Containerization Packaging the HR Management System application (Java Spring Boot) into a Docker Container to ensure a unified running environment between Dev/Test/Prod (Dev = Prod), limiting configuration errors and making it easy to develop declarations. Deployment Deploying Docker Container to AWS Elastic Beanstalk. Elastic Beanstalk will automatically: Provision EC2 Instance Levels and Configuration Set up Application Load Balancing Create Auto Scaling Groups to automatically scale under load Manage application lifecycles without manual intervention "},{"uri":"https://thienluhoan.github.io/workshop-template/5-workshop/5.5-policy/5.5.2-initialize-elastic-beanstalk/","title":"Initialize Elastic Beanstalk","tags":[],"description":"","content":" We will create an environment to run the application Access Elastic Beanstalk \u0026gt; Create application )\nApp Name: MiniMarket-App Platform: Docker (Amazon Linux 2023) Application code: Select Sample application (To test infrastructure first) ) ) Network Configuration (Networking) - Extremely Important: VPC: Select the VPC you created for MiniMarket\nInstance settings:\nPublic IP address: Uncheck\nSubnets: Select 2 Private Subnets\nEC2 security groups: Select sg-web-app ) )\nCapacity:\nEnvironment type: Select Load balanced )\nLoad balancer network settings:\nVisibility: Public Subnets: Select 2 Public Subnets ) Click Create environment and wait for environment to be ready "},{"uri":"https://thienluhoan.github.io/workshop-template/5-workshop/5.5-policy/5.5.1-package-with-docker/","title":"Package with Docker","tags":[],"description":"","content":" Before moving to Cloud, we need to package the .NET Core application into a Docker Image\nCreate Dockerfile: At the root directory of the Solution, create a file named Dockerfile (no extension) 2. Create buildspec.yml: Create file buildspec.yml to instruct AWS CodeBuild how to package and push to ECR\n"},{"uri":"https://thienluhoan.github.io/workshop-template/1-worklog/1.5-week5/","title":"Week 5 Worklog","tags":[],"description":"","content":"Week 5 Objectives: Understand the Shared Responsibility Model and security responsibilities between AWS and customers. Master Identity and Access Management (IAM) mechanisms — users, groups, roles, policies. Learn about AWS Organization, AWS Identity Center, and AWS KMS in security management. Know how to authorize, encrypt, and audit security in AWS environment. Familiarize with Amazon Cognito and AWS Security Hub. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 Learn about Shared Responsibility Model: division of responsibilities between AWS and customers; service classification by management level (Infrastructure, Hybrid, Fully-managed). 08/09/2025 08/09/2025 https://cloudjourney.awsstudygroup.com/ 3 Study Root Account and best practices: create IAM admin user, lock root credentials, protect domain/email information. 09/09/2025 09/09/2025 https://cloudjourney.awsstudygroup.com/ 4 Learn about IAM Users, Groups, Roles, Policies — authorization mechanisms, trust policy, explicit deny. 10/09/2025 10/09/2025 https://cloudjourney.awsstudygroup.com/ 5 Practice creating IAM Role, assume role, and using AWS STS (Security Token Service) to grant temporary permissions. 11/09/2025 11/09/2025 https://cloudjourney.awsstudygroup.com/ 6 Learn about Amazon Cognito (User Pool \u0026amp; Identity Pool) — login, authentication, and AWS resource access mechanisms. 12/09/2025 12/09/2025 https://cloudjourney.awsstudygroup.com/ 7 Study AWS Organization, Identity Center, KMS, and Security Hub — centralized management, data encryption, automated security assessment. 13/09/2025 13/09/2025 https://cloudjourney.awsstudygroup.com/ Week 5 Achievements: Shared Responsibility Model AWS is responsible for security of the cloud — hardware, infrastructure, hypervisor. Customers are responsible for security in the cloud — service configuration, data, encryption, user management. Level of responsibility varies by service type: Infrastructure (EC2, VPC, EBS) → customers bear most. Managed Services (RDS, EKS) → shared. Fully Managed (S3, Lambda) → AWS bears more. AWS Root Account Has full access to all AWS services and resources. Should lock credentials, only use in emergencies. Best practices: Create IAM Administrator user to replace root. Split root access between 2 different administrators. Ensure domain and email root account renewal regularly. AWS Identity and Access Management (IAM) Service that controls access to AWS resources. Principals include: AWS Root user, IAM user, Federated user (via SAML/OAuth), IAM role, AWS service, Anonymous user. IAM User: No default permissions. Can log in via Management Console or use Access Key/Secret Key. Not used to manage applications/operating systems. IAM Group: Groups users for easier management (nested groups not supported). IAM Policy (JSON): Identity-based policy (attached to Principal). Resource-based policy (attached to Resource). Always prioritize explicit Deny \u0026gt; Allow. IAM Role: No fixed credentials, only used when assumed. Trust policy determines who can assume the role. Used for cross-account access or granting permissions to AWS services (like EC2). Linked with AWS STS to provide temporary credentials. Amazon Cognito Service for authentication and user management for web/mobile apps. Consists of 2 components: User Pool – stores, registers, and authenticates users. Identity Pool – grants access to AWS services. Supports login via username/password or third-party (Facebook, Google, Amazon). User Pool combined with Identity Pool to directly access AWS resources after authentication. AWS Organization Allows management of multiple AWS accounts within the same organization. Supports OU (Organizational Units) and Consolidated Billing. Can assign Service Control Policy (SCP) to set maximum permission limits on OU/account. SCP is deny-based, does not grant permissions but only sets boundaries. AWS Identity Center (formerly SSO) Manages access for multiple AWS accounts and external applications. Identity source can be: AWS Managed AD On-premises AD (via Trust/Connector) Permission Set: set of permissions assigned to user/group → grants role to accounts in Organization. AWS Key Management Service (KMS) Service that creates and manages encryption keys to encrypt data. Supports Encryption at Rest meeting FIPS 140-2 standards. CMK (Customer Managed Key) commonly used to create Data Key, serving actual data encryption. AWS never exports CMK out of the system. AWS Security Hub Service that automatically audits security based on AWS Best Practices and industry standards. Runs continuously, evaluates service configurations and displays security score. Supports aggregating security alerts from multiple other services (GuardDuty, Config, Inspector\u0026hellip;). "},{"uri":"https://thienluhoan.github.io/workshop-template/5-workshop/","title":"Workshop","tags":[],"description":"","content":"Secure Hybrid Access to S3 using VPC Endpoints Overview AWS PrivateLink provides private connectivity to AWS services from VPCs and your on-premises networks, without exposing your traffic to the Public Internet.\nIn this lab, you will learn how to create, configure, and test VPC endpoints that enable your workloads to reach AWS services without traversing the Public Internet.\nYou will create two types of endpoints to access Amazon S3: a Gateway VPC endpoint, and an Interface VPC endpoint. These two types of VPC endpoints offer different benefits depending on if you are accessing Amazon S3 from the cloud or your on-premises location\nGateway - Create a gateway endpoint to send traffic to Amazon S3 or DynamoDB using private IP addresses.You route traffic from your VPC to the gateway endpoint using route tables. Interface - Create an interface endpoint to send traffic to endpoint services that use a Network Load Balancer to distribute traffic. Traffic destined for the endpoint service is resolved using DNS. Content Workshop overview Prerequiste Network Infrastructure Setup Application Deployment CI/CD Automation) Optimization \u0026amp; Security Cleanup "},{"uri":"https://thienluhoan.github.io/workshop-template/5-workshop/5.6-monitoringoperations/","title":"Monitoring &amp; Operations","tags":[],"description":"","content":"Overview A Production system cannot be considered complete without Monitoring and Alerting capabilities. You cannot sit watching the screen 24/7 to check if the Server is still alive\nIn this module, we will set up monitoring and alerting for MiniMarket using AWS operations management services:\nAmazon CloudWatch: Collect metrics (Metrics) from EC2, RDS, ELB\nAmazon SNS (Simple Notification Service): Notification service. We will use it to send Emails to administrators when the system encounters issues\nWe will set up a CloudWatch Alarm to monitor Web Server CPU. If CPU exceeds 70% (sign of overload or attack), the system will automatically trigger SNS to send emergency alert Email "},{"uri":"https://thienluhoan.github.io/workshop-template/6-self-evaluation/","title":"Self-Assessment","tags":[],"description":"","content":"During my internship at Amazon Web Services Vietnam Co., Ltd. from December 8, 2025 to December 11, 2025, I had the opportunity to learn, practice, and apply the knowledge acquired in school to a real-world working environment.\nI participated in the role of Backend Developer (Java Spring Boot) in the Human Resource Management System project with the main tasks:\nDevelop and build APIs using Spring Boot to serve human resource management functions such as timekeeping, salary, contracts, departments, and user authorization Integrate facial recognition system using Python to automate employee attendance and identity authentication Combine user interface and AI to ensure data is processed accurately, synchronously, and securely Participate in request analysis, optimize API performance, and support error handling during the development process Through this experience, I have improved my skills in:\nProgramming Skills: Java 21, Spring Boot 3.5.6, RESTful API Development, Spring Data JPA/Hibernate, SQL (PostgreSQL, MySQL), Spring Security, JWT Authentication \u0026amp; Authorization, DTO Pattern, Exception Handling, API Documentation (Swagger/OpenAPI)\nAnalysis \u0026amp; System Design: Business Logic Implementation (Payroll, Attendance, Leave Management), Data Processing \u0026amp; Aggregation (Java Stream API, complex SQL queries), Problem-Solving \u0026amp; Algorithm Design, Requirements Analysis \u0026amp; System Design\nReporting \u0026amp; Analytics: Statistics \u0026amp; Analytics Development, Building Dashboard \u0026amp; Reporting Endpoints, Data Aggregation \u0026amp; Visualization, Financial Reporting \u0026amp; Payroll Statistics\nCommunication \u0026amp; Collaboration: Technical Documentation (API Docs, JavaDoc), Code Review \u0026amp; Team Collaboration, Stakeholder Communication, Requirements Translation (Business ↔ Technical)\nAdditional Skills: Security (JWT, Spring Security, RBAC, Policy-Based Access Control), Cloud (AWS: S3, SQS, SES, KMS, Secrets Manager), Testing (JUnit 5, Mockito, Integration Testing), Architecture (Layered Architecture, Design Patterns: Mapper, Repository, Builder), Tools (Maven, Git, Postman, DBeaver/PGAdmin/Workbench), Domain Knowledge (HR Management, Payroll Processing, Attendance \u0026amp; Financial Calculations)\nIn terms of work ethic, I always strived to complete tasks well, complied with workplace regulations, and actively engaged with colleagues to improve work efficiency.\nTo objectively reflect on my internship period, I would like to evaluate myself based on the following criteria:\nNo. Criteria Description Good Fair Average 1 Professional knowledge \u0026amp; skills Understanding of the field, applying knowledge in practice, proficiency with tools, work quality ☐ ✅ ☐ 2 Ability to learn Ability to absorb new knowledge and learn quickly ☐ ✅ ☐ 3 Proactiveness Taking initiative, seeking out tasks without waiting for instructions ☐ ✅ ☐ 4 Sense of responsibility Completing tasks on time and ensuring quality ☐ ✅ ☐ 5 Discipline Adhering to schedules, rules, and work processes ☐ ✅ ☐ 6 Progressive mindset Willingness to receive feedback and improve oneself ☐ ✅ ☐ 7 Communication Presenting ideas and reporting work clearly ☐ ✅ ☐ 8 Teamwork Working effectively with colleagues and participating in teams ☐ ✅ ☐ 9 Professional conduct Respecting colleagues, partners, and the work environment ☐ ✅ ☐ 10 Problem-solving skills Identifying problems, proposing solutions, and showing creativity ☐ ✅ ☐ 11 Contribution to project/team Work effectiveness, innovative ideas, recognition from the team ☐ ✅ ☐ 12 Overall General evaluation of the entire internship period ☐ ✅ ☐ Needs Improvement Strengthen discipline and strictly comply with the rules and regulations of the company or any organization Improve problem-solving thinking Enhance communication skills in both daily interactions and professional contexts, including handling situations effectively "},{"uri":"https://thienluhoan.github.io/workshop-template/1-worklog/1.6-week6/","title":"Week 6 Worklog","tags":[],"description":"","content":"Week 6 Objectives: Understand basic concepts of Database and RDBMS / NoSQL. Distinguish OLTP, OLAP and corresponding data systems. Master the structure and role of Primary Key, Foreign Key, Index, Partition, Buffer, Log. Learn about AWS database services such as Amazon RDS, Aurora, Redshift, ElastiCache. Know how to optimize, recover, scale, and secure databases on AWS platform. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 Review database concepts: Database, Session, Primary/Foreign Key, Index, Partition, Buffer, Execution Plan, DB Log. 15/09/2025 15/09/2025 https://cloudjourney.awsstudygroup.com/ 3 Learn about RDBMS (relational model, SQL) and NoSQL (Document, Key-Value, Graph, Wide-column). 16/09/2025 16/09/2025 https://cloudjourney.awsstudygroup.com/ 4 Compare OLTP and OLAP, identify suitable application types and role of data warehouse. 17/09/2025 17/09/2025 https://cloudjourney.awsstudygroup.com/ 5 Learn about Amazon RDS: architecture, features (backup, read replica, failover, scaling, encryption). 18/09/2025 18/09/2025 https://cloudjourney.awsstudygroup.com/ 6 Learn about Amazon Aurora: read/write performance, backtrack, clone, global DB, multi-master. 19/09/2025 19/09/2025 https://cloudjourney.awsstudygroup.com/ 7 Learn about Amazon Redshift and ElastiCache, applications for OLAP and caching. 20/09/2025 20/09/2025 https://cloudjourney.awsstudygroup.com/ Week 6 Achievements: 🧩 Database Concept Database: structured/semi-structured information system, stored on devices to serve simultaneous retrieval from multiple applications. Session: time period from connection to disconnection with the database system. Primary Key: uniquely identifies each record in a table. Foreign Key: link between tables through reference to primary key. Index: data structure that helps speed up retrieval, but costs memory and write overhead. Partition: divides large tables into multiple parts to query faster. Execution Plan: query execution plan created by query optimizer to choose the most efficient approach. DB Log: records changes to help recovery and synchronization between primary–replica. Buffer: temporary memory area to speed up read/write data before syncing with disk. RDBMS \u0026amp; NoSQL RDBMS (Relational Database Management System):\nData organized by tables, rows, columns; relationships between tables represented by keys. Uses SQL to query and manage data. Ensures data integrity, supports ACID (Atomicity, Consistency, Isolation, Durability). NoSQL (Not only SQL):\nDoes not store data in table format, has flexible structure. Common types: Document-based: MongoDB. Key-Value: Redis, DynamoDB. Wide-column: Cassandra. Graph: Neo4j. Suitable for big data, unstructured, flexible scaling applications. OLTP vs OLAP Feature OLTP OLAP Purpose Real-time transaction processing Historical data analysis Data Frequently updated Aggregated, read-only Application Banking, retail, booking Reporting, BI, analytics Focus Transaction processing speed Query response speed AWS Technology RDS, Aurora Redshift, Athena, QuickSight Amazon RDS (Relational Database Service) Fully managed database service. Supports: Aurora, MySQL, PostgreSQL, MariaDB, Oracle, MSSQL. Key features: Automatic backup (DB + Log, keep up to 35 days). Read Replica supports read workload (reporting). Automatic failover between Primary/Standby (Multi-AZ). Encryption at rest \u0026amp; in transit. Auto scaling storage \u0026amp; instance size. Security with Security Group and NACL. Commonly used for OLTP applications. Amazon Aurora High-performance optimized RDBMS, compatible with MySQL and PostgreSQL. Inherits all RDS features, adds characteristics: Backtrack – restore DB to previous point in time. Clone – create fast copy. Global Database – multiple regions with 1 master, multiple read replicas. Multi-master – supports parallel writes from multiple nodes. Data stored distributed and automatically synchronized across multiple AZs. Amazon Redshift Data Warehouse service managed by AWS, core is PostgreSQL optimized for OLAP. Uses Massively Parallel Processing (MPP) architecture: Leader Node coordinates queries. Compute Node stores and processes data. Stores data columnar (Columnar Storage) → optimized for analytics. Supports: SQL, JDBC, ODBC. Redshift Spectrum – query data directly in S3. Transient cluster – cost savings when paused. Amazon ElastiCache Fully managed caching engine service. Supports two engines: Redis and Memcached. Helps reduce database load, improve data access performance. AWS automatically detects and replaces failed nodes. Redis generally recommended more due to: Rich features (replication, persistence, pub/sub). High performance and good scalability. Need to manage caching logic in application to ensure data consistency. "},{"uri":"https://thienluhoan.github.io/workshop-template/5-workshop/5.7-clean-up/","title":"Clean Up","tags":[],"description":"","content":"Overview Congratulations on successfully developing HRM on AWS!\nHowever, our work does not stop there. The final and most important step to protect your \u0026ldquo;wallet\u0026rdquo; is Resource Cleanup.\nThe services we develop such as NAT Gateway, Elastic Load Balancer, RDS, ElastiCache are all charged by the hour, regardless of whether you use them or not. If you forget to delete them, the bill at the end of the month can be very high.\nWe will perform the system dismantling process in the correct order to ensure that we do not miss any causes of hidden costs.\n"},{"uri":"https://thienluhoan.github.io/workshop-template/7-feedback/","title":"Sharing and Feedback","tags":[],"description":"","content":" Here, you can freely share your personal opinions about your experience participating in the First Cloud Journey program. This will help the FCJ team improve any shortcomings based on the following aspects:\nInternship Evaluation - From an FCJer\u0026rsquo;s Perspective Overall Evaluation 1. Working Environment Throughout my internship at FCJ, I clearly felt that this is a friendly, open, and professional working environment. From the very first day, I was warmly welcomed and introduced to the team, which immediately made me feel comfortable and integrated. Team members are always ready to help whenever I encounter difficulties, whether during or outside working hours. I remember many times when I had questions late in the evening, and the trainers still took time to answer and guide me patiently.\nThe workspace is comfortable and tidy, with modern equipment and a well-organized layout that helps me easily focus and maintain productivity. The office atmosphere is always positive, with everyone maintaining a professional yet relaxed attitude. The open communication culture allows me to freely express my opinions and ask questions without hesitation.\nHowever, I believe that if the company could add more team building activities or internal social gatherings, such as monthly team lunches, coffee breaks, or quarterly team outings, the bonding spirit among members would be even better. These activities would not only strengthen relationships but also create more opportunities for knowledge sharing and collaboration across different teams.\n2. Support from Mentor \u0026amp; Trainer As an intern with limited experience, I was fortunate to learn from many dedicated trainers such as Anh Kha, Anh Thịnh, Sư phụ Hưng, Anh Hoàng, Anh Vĩ, Anh Thiện, Chị Linh Nhi, along with the Little Member FCJ members. Each trainer brought their unique expertise and teaching style, which enriched my learning experience in different ways.\nAnh Kha always provided clear, structured guidance and was patient in explaining complex concepts. Anh Thịnh shared many practical insights from real-world projects. Sư phụ Hưng\u0026rsquo;s deep technical knowledge and systematic approach helped me understand the fundamentals thoroughly. Anh Hoàng\u0026rsquo;s enthusiasm and encouragement motivated me to push beyond my comfort zone. Anh Vĩ\u0026rsquo;s attention to detail and best practices taught me the importance of quality work. Anh Thiện\u0026rsquo;s problem-solving approach showed me different ways to tackle challenges. Chị Linh Nhi\u0026rsquo;s organizational skills and support helped me manage my tasks effectively. The Little Member FCJ members, despite being peers, were always willing to share their experiences and help me navigate through difficulties.\nEveryone not only provided detailed guidance for each task but also thoroughly explained foundational knowledge, helping me understand deeply rather than just following mechanically. I especially appreciate that the trainers always created conditions for me to find solutions to problems myself before providing answers – this helped me grow significantly in thinking and skills. For instance, when I encountered a technical issue, instead of immediately giving me the solution, they would guide me with questions like \u0026ldquo;What have you tried?\u0026rdquo; or \u0026ldquo;What do you think might be the cause?\u0026rdquo; This approach developed my analytical thinking and problem-solving abilities.\n3. Relevance of Work to Academic Major The tasks assigned to me align well with my academic major in Information Technology, while also opening up many new areas of knowledge that I had never encountered before. The work covered various aspects including cloud computing, DevOps practices, AI-driven development, IAM security, and system architecture – all of which are highly relevant to modern software development.\nFor example, working on AWS services helped me apply theoretical knowledge about distributed systems and cloud architecture that I learned in university. The DevOps tasks allowed me to practice automation and CI/CD concepts that were only briefly mentioned in coursework. The AI-related projects introduced me to practical applications of machine learning and generative AI, which are cutting-edge technologies in the industry.\nThanks to this, I was able to both strengthen my foundational knowledge and learn practical skills for future work. The combination of theoretical understanding from university and hands-on experience from the internship created a comprehensive learning experience that prepared me well for my future career.\n4. Learning \u0026amp; Skill Development Opportunities The internship helped me develop many important skills that are essential for professional growth:\nTechnical Skills:\nUsing project management tools like Jira, Trello, or similar platforms to track tasks and collaborate with team members Working with AWS services including EC2, S3, Lambda, IAM, and various other cloud services Understanding DevOps practices including CI/CD pipelines, infrastructure as code, and automation Learning about AI tools and their integration into development workflows Gaining experience with version control systems and collaborative coding practices Soft Skills:\nTeamwork skills: Learning to collaborate effectively with team members, understanding different working styles, and contributing to group projects Professional communication skills: Writing clear technical documentation, presenting ideas in meetings, and communicating effectively with both technical and non-technical stakeholders Time management: Balancing multiple tasks, prioritizing work, and meeting deadlines Real-world problem-solving thinking: Approaching complex problems systematically, breaking them down into manageable parts, and finding creative solutions Professional Development:\nUnderstanding industry best practices and standards Learning about career paths in cloud computing and software development Gaining insights into how large-scale projects are managed Developing a professional mindset and work ethic In addition, the sharing from mentors and trainers about work experience and career orientation also helped me greatly in determining my own development path. They shared stories about their career journeys, challenges they faced, and how they overcame them. These insights were invaluable in helping me understand what to expect in my future career and how to prepare for it.\n5. Company Culture \u0026amp; Team Spirit FCJ culture emphasizes respect, support, and responsibility. From day one, I noticed that everyone treats each other with respect, regardless of position or experience level. Senior members are approachable and willing to share knowledge, while everyone supports each other in achieving common goals.\nAlthough work intensity can sometimes be high, especially during project deadlines or important events, everyone always coordinates smoothly, supports each other, and maintains a cheerful atmosphere during work. I observed that when someone faces a challenge, others immediately step in to help. This collaborative spirit creates a positive work environment where everyone can thrive.\nThe team spirit is particularly evident during team meetings and collaborative sessions. Everyone\u0026rsquo;s opinions are valued, and there\u0026rsquo;s a genuine sense of collective ownership of projects. Even as an intern, my contributions were acknowledged and appreciated, which made me feel valued and motivated to do my best.\nThis made me – even as an intern – always feel like I am part of the team. I never felt like an outsider or someone who was just there temporarily. The inclusive culture and supportive environment made my internship experience truly meaningful and enjoyable.\n6. Internship Policies \u0026amp; Benefits FCJ provides a fair internship allowance that recognizes the value of interns\u0026rsquo; contributions. Additionally, the company offers flexible time arrangements when I need adjustments, which is extremely helpful for balancing between study and work. This flexibility allowed me to attend important classes, complete assignments, and still fulfill my internship responsibilities effectively.\nThe opportunity to participate in internal training sessions is also a great value that I highly appreciate. These sessions covered various topics from technical skills to soft skills, and they were conducted by experienced professionals. The training not only enhanced my knowledge but also provided networking opportunities with other interns and team members.\nOther benefits that made the internship experience positive include:\nAccess to learning resources and documentation Opportunities to attend external events and workshops Support for professional development activities Recognition and feedback on work performance A supportive environment that encourages learning and growth These policies and benefits demonstrate that FCJ values its interns and is committed to providing a meaningful learning experience rather than just using interns for basic tasks.\nAdditional Questions 1. What did you find most satisfying during your internship? What satisfied me most was the opportunity to learn directly from experienced trainers and the enthusiastic support from the entire team. However, there were several specific aspects that made this experience particularly satisfying:\nLearning from Experts: Being able to work alongside and learn from professionals who have years of experience in the industry was incredibly valuable. Each trainer brought unique perspectives and knowledge that I couldn\u0026rsquo;t have gained from textbooks or online courses alone.\nReal-World Application: The satisfaction of applying what I learned to actual projects and seeing tangible results was immensely rewarding. Whether it was deploying a cloud infrastructure, implementing security best practices, or contributing to a team project, each accomplishment gave me a sense of achievement.\nPersonal Growth: Watching myself grow from someone who was hesitant and unsure to someone who could confidently tackle challenges and contribute meaningfully to projects was deeply satisfying. The transformation in my skills, confidence, and professional mindset was remarkable.\nSupportive Environment: The fact that I could make mistakes, ask questions, and learn without fear of judgment created an environment where I could truly grow. The team\u0026rsquo;s patience and encouragement made every learning moment enjoyable rather than stressful.\nMeaningful Work: Knowing that my contributions, even as an intern, were valued and made a difference to the team and projects was highly satisfying. I never felt like I was doing busy work; every task had purpose and meaning.\n2. What do you think the company should improve? While my overall experience was excellent, there are a few areas where the company could make improvements to enhance the internship program even further:\nMore Group Activities: The company could enhance more group activities and cross-departmental experience-sharing workshops to help interns gain broader perspectives. Regular team building activities, social events, or informal gatherings would strengthen relationships and create a more cohesive team environment.\nStructured Mentorship Program: While the support was excellent, having a more structured mentorship program with regular one-on-one sessions, goal setting, and progress reviews could provide even more focused guidance for interns\u0026rsquo; development.\nAdvanced Training Opportunities: Offering more advanced training courses on professional skills, specialized technical topics, or industry certifications would help interns develop deeper expertise in areas of interest.\nCross-Functional Exposure: Providing opportunities for interns to work with different teams or departments would give them a broader understanding of how the organization works and help them discover their interests and strengths.\nFeedback Mechanisms: While feedback was provided, having more formal and structured feedback sessions with documented development plans could help interns track their progress and identify areas for improvement more systematically.\nResource Library: Creating a centralized resource library with documentation, best practices, tutorials, and learning materials would be valuable for interns to reference and continue learning independently.\n3. Would you recommend your friends to intern here? Why? Absolutely yes. I would wholeheartedly recommend FCJ to my friends who are looking for a meaningful internship experience. Here\u0026rsquo;s why:\nExcellent Learning Environment: FCJ is an environment suitable for those who want to learn, develop practical skills, and work with friendly, enthusiastic colleagues. The combination of experienced mentors, real-world projects, and supportive team members creates an ideal learning environment.\nProfessional Growth: The internship provides opportunities for genuine professional growth. Interns are given meaningful work, not just menial tasks, which allows them to build real skills and experience that will be valuable in their future careers.\nSupportive Culture: The friendly and supportive culture makes it easy for interns to ask questions, make mistakes, and learn without fear. This is particularly important for students who might be nervous about their first professional experience.\nIndustry Relevance: Working with cutting-edge technologies like AWS cloud services, AI tools, and modern development practices gives interns exposure to industry-standard tools and methodologies that are highly relevant in today\u0026rsquo;s job market.\nNetworking Opportunities: The opportunity to work with experienced professionals and build relationships within the industry is invaluable. These connections can be beneficial for future career opportunities.\nPositive Experience: My personal experience was overwhelmingly positive, and I believe others would benefit similarly from this program. The combination of learning, growth, and positive work environment makes FCJ an excellent choice for an internship.\nI would especially recommend it to friends who are serious about learning, willing to work hard, and looking for an internship that will truly prepare them for their future careers rather than just being a resume filler.\nSuggestions \u0026amp; Expectations Training and Development:\nCould open more advanced training courses on professional skills, including specialized technical topics, industry certifications, and soft skills development. Consider offering certification preparation courses or study groups for AWS certifications or other relevant professional certifications. Provide more workshops on emerging technologies and industry trends to keep interns updated with the latest developments. Program Structure:\nConsider implementing a more structured onboarding process with clear learning objectives and milestones for the internship period. Create a mentorship matching system that pairs interns with mentors based on interests and career goals. Develop a comprehensive internship handbook or guide that outlines expectations, resources, and opportunities available to interns. Future Engagement:\nIf given the opportunity, I would very much like to continue accompanying and staying long-term with the program in the future. Whether as a returning intern, part-time contributor, or full-time team member, I would be honored to continue being part of the FCJ community. I hope to maintain connections with the team and continue learning from the experienced professionals I\u0026rsquo;ve had the privilege to work with. Long-term Vision:\nI believe FCJ has the potential to become a leading internship program that sets the standard for how companies should engage with and develop young talent. I hope the program continues to grow and expand, offering opportunities to more students while maintaining the high quality of mentorship and support that makes it special. Other Sharing My internship at FCJ was an extremely valuable time for me, and I would like to share some final thoughts:\nGratitude: I would like to send sincere thanks to all the trainers who took time to guide, support, and inspire me to improve every day. Your patience, expertise, and encouragement made this experience truly transformative. Thank you to Anh Kha, Anh Thịnh, Sư phụ Hưng, Anh Hoàng, Anh Vĩ, Anh Thiện, Chị Linh Nhi, and all the Little Member FCJ members who made me feel welcome and supported throughout my journey.\nReflection: Looking back on my internship, I can see how much I\u0026rsquo;ve grown both professionally and personally. The skills I\u0026rsquo;ve gained, the relationships I\u0026rsquo;ve built, and the confidence I\u0026rsquo;ve developed will serve me well in my future career. This experience has not only enhanced my resume but has fundamentally shaped my understanding of what it means to be a professional in the technology industry.\nImpact: The impact of this internship extends beyond just the technical skills I learned. It has influenced my career goals, my approach to problem-solving, and my understanding of teamwork and collaboration. The lessons I\u0026rsquo;ve learned here will stay with me throughout my professional journey.\nRecommendation: For any student considering an internship, I would strongly recommend FCJ. It\u0026rsquo;s not just about the work or the learning – it\u0026rsquo;s about being part of a community that genuinely cares about your growth and development. The investment that FCJ makes in its interns is evident in every interaction, every training session, and every project.\nFuture Hopes: I hope to stay connected with the FCJ community and continue learning from the amazing professionals I\u0026rsquo;ve met here. I also hope that future interns will have the same positive experience I had and that the program continues to evolve and improve based on feedback and changing industry needs.\nThank you, FCJ, for an unforgettable internship experience that has truly made a difference in my professional development and personal growth.\n"},{"uri":"https://thienluhoan.github.io/workshop-template/1-worklog/1.7-week7/","title":"Week 7 Worklog","tags":[],"description":"","content":"🎯 Week 7 Objectives: Continue to enhance AWS foundational knowledge to serve the following specialized weeks. Proficiently operate AWS Console and AWS CLI instead of just watching theory. Master EC2 – the core service that will be used throughout the internship journey. 📌 Tasks to be carried out this week Day Task Start Date Completion Date Reference Material 2 - Update progress with mentor - Receive week 7 objectives - Prepare environment to start AWS practice 11/08/2025 11/08/2025 3 - Review AWS knowledge learned last week - Learn about main service groups: Compute, Storage, Networking, Database 12/08/2025 12/08/2025 https://cloudjourney.awsstudygroup.com/ 4 - Practice: + Create AWS Free Tier account + Install and configure AWS CLI + Test CLI operation through test commands 13/08/2025 13/08/2025 https://cloudjourney.awsstudygroup.com/ 5 - Study EC2 in more detail including: + Instance type + AMI + EBS volume + Elastic IP - Watch real-world EC2 use cases in enterprises 14/08/2025 15/08/2025 https://cloudjourney.awsstudygroup.com/ 6 - Practice: + Create first EC2 instance + Connect via SSH using key-pair + Attach and manage EBS Volume + Try basic server management operations 15/08/2025 15/08/2025 https://cloudjourney.awsstudygroup.com/ ✅ Week 7 Achievements: Understand the role of AWS in system development and begin to proficiently use basic service groups:\nCompute (EC2) Storage (S3, EBS) Networking (Basic VPC) Database (RDS Overview) Created and configured AWS Free Tier account without errors, managed access key, secret key, region.\nInstalled and operated AWS CLI smoothly:\nConfigure profile information Check resources via CLI Get list of regions, AMI, EC2 instances Manage key pairs Proficient in basic EC2 operations:\nCreate EC2 instance SSH connect to Linux server Create – attach – detach EBS volume Assign Elastic IP and basic network management Confident in using both web interface (AWS Console) and CLI in parallel to manage resources.\nCompleted all week 7 objectives as required by mentor.\n📌 Personal Notes: Week 7 is a pivotal week to start moving to heavier content like deep networking, IAM, autoscaling… Hands-on CLI operations helped me understand the nature of services better compared to just using Console. This is also the first time managing a real EC2 server, so I gained a lot of practical experience.\n"},{"uri":"https://thienluhoan.github.io/workshop-template/1-worklog/1.8-week8/","title":"Week 8 Worklog","tags":[],"description":"","content":"🎯 Week 8 Objectives: Master knowledge about AWS VPC, network security, subnet structure. Know how to design basic network systems in AWS. Practice creating VPC, subnet, security group, route table and test connectivity. 📌 Tasks to be carried out this week Day Task Start Date Completion Date Reference Material 2 - Review EC2 knowledge from last week - Receive week 8 learning objectives from mentor 18/08/2025 18/08/2025 3 - Learn about VPC concepts: + CIDR + Subnet (Public/Private) + Internet Gateway + NAT Gateway 19/08/2025 19/08/2025 https://cloudjourney.awsstudygroup.com/ 4 - Deep dive into network security: + Security Group + Network ACL (NACL) + Route Table 20/08/2025 20/08/2025 https://cloudjourney.awsstudygroup.com/ 5 - Practice: + Create new VPC + Create Public/Private Subnet + Create and bind Internet Gateway, NAT Gateway + Update Route Table 21/08/2025 22/08/2025 https://cloudjourney.awsstudygroup.com/ 6 - Testing: + Deploy EC2 to Public Subnet and SSH from Internet + Deploy EC2 to Private Subnet and test Internet access via NAT 22/08/2025 22/08/2025 https://cloudjourney.awsstudygroup.com/ ✅ Week 8 Achievements: Understand AWS VPC network architecture including:\nIP range – CIDR Public and Private Subnet Internet Gateway and NAT Gateway Route Table – routing mechanism between subnets Apply network security knowledge:\nSet up appropriate Security Group (SSH, HTTP, HTTPS) Understand inbound/outbound rule mechanism Know how to distinguish SG and NACL Practice building a complete VPC environment:\n01 VPC 01 Public Subnet + 01 Private Subnet 01 Internet Gateway + 01 NAT Gateway Manually configured Route Table EC2 operating stably in each subnet Successfully tested independently:\nSSH into EC2 in Public Subnet Private Subnet cannot SSH directly from Internet Private Subnet accesses Internet via NAT Gateway Begin to understand \u0026ldquo;infrastructure as network topology\u0026rdquo; thinking — how cloud separates resources by network layer.\n📌 Personal Notes: Week 8 helped me understand more about network systems in AWS — the most important part when deploying backend applications. Hands-on VPC creation and connection testing helped me visualize how enterprises design secure systems, limiting risks. This is an important step to prepare for advanced content next week like Load Balancer, Auto Scaling, IAM, S3, and serverless services.\n"},{"uri":"https://thienluhoan.github.io/workshop-template/1-worklog/1.9-week9/","title":"Week 9 Worklog","tags":[],"description":"","content":"🎯 Week 9 Objectives: Understand network architecture in AWS more deeply (VPC, Subnet, Routing). Familiarize with standard infrastructure design for Spring Boot backend projects. Practice building infrastructure platform for Human Resource Management system (HRM Project). 📌 Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Meeting with mentor / trainer to review knowledge learned in week 7. - Receive week 8 roadmap (VPC – Network – Security). 25/08/2025 25/08/2025 3 - Learn about AWS VPC: + CIDR, IP Addressing + Public / Private Subnet + Route Table + Internet Gateway 26/08/2025 26/08/2025 https://cloudjourney.awsstudygroup.com/ 4 - Study AWS network security mechanisms: + Security Group + Network ACL + Differences SG vs NACL 27/08/2025 27/08/2025 https://cloudjourney.awsstudygroup.com/ 5 - Practice: + Create new VPC for HRM project + Create Public Subnet + Private Subnet + Attach Internet Gateway to Public Subnet 28/08/2025 29/08/2025 AWS Console / AWS Docs 6 - Advanced practice: + Create NAT Gateway for Private Subnet + Create test EC2 in each subnet + Test SSH, outbound Internet and routing 29/08/2025 29/08/2025 AWS Console / CLI ✅ Week 9 Achievements: Understand AWS VPC network structure and role of each component:\nCIDR → manage IP range. Public subnet → for resources needing internet (jump server). Private subnet → deploy backend, database. Route Table → route traffic. Internet Gateway \u0026amp; NAT Gateway → manage inbound/outbound traffic. Distinguish and use:\nSecurity Group (stateful inbound/outbound rules). Network ACL (stateless firewall layer deeper than SG). Successfully built a standard VPC environment for the project:\n01 dedicated VPC. 02 subnets (Public / Private). 01 Internet Gateway + 01 NAT Gateway. 02 test EC2 instances → functioning correctly in each subnet. Successfully tested:\nEC2 Public Subnet → direct SSH. EC2 Private Subnet → cannot SSH from internet (correct design). Private Subnet can access Internet via NAT. Begin to understand how AWS networking serves Spring Boot HRM application deployment later:\nBackend runs in Private Subnet → higher security. Only public Load Balancer is accessible to users. Database completely private. 📌 Personal Notes: Week 9 helped me understand network layer more deeply — the most important part when building enterprise applications on AWS platform. Hands-on VPC configuration and connection testing helped me understand \u0026ldquo;secure-by-design\u0026rdquo; thinking and how AWS technical teams deploy real systems. This is the foundation to move to next week\n"},{"uri":"https://thienluhoan.github.io/workshop-template/1-worklog/1.10-week10/","title":"Week 10 Worklog","tags":[],"description":"","content":"🎯 Week 10 Objectives: Complete HRM backend system at the level of connecting with AWS infrastructure. Learn and deploy AWS IAM, standard access permissions for the project. Integrate AWS services such as S3, SES, Secrets Manager into project. Add basic logging – monitoring. 📌 Tasks to be carried out this week: Day Task Start Date Completion Date Documentation 2 - Review week 9 results (ALB + Auto Scaling). - Receive week 10 requirements from mentor. 08/09/2025 08/09/2025 3 - Learn about IAM: + IAM User / Role / Group + Inline Policy vs Managed Policy + Best Practices \u0026ldquo;Least Privilege\u0026rdquo; 09/09/2025 09/09/2025 https://cloudjourney.awsstudygroup.com/ 4 - IAM Practice: + Create IAM Role for EC2 + Grant S3, Secrets Manager access permissions + Configure IAM User for dev/test 10/09/2025 10/09/2025 AWS Docs 5 - Learn about AWS S3 \u0026amp; integrate into HRM backend: + Upload employee avatar + Retrieve files with signed URL + Set up Bucket Policy, CORS 11/09/2025 11/09/2025 AWS S3 Docs 6 - Learn about Secrets Manager: + Store database credentials + Get secret in Spring Boot (SDK) - Learn about SES to prepare for sending HR notification emails 12/09/2025 12/09/2025 AWS Docs ✅ Week 10 Achievements: 🔐 IAM (Identity \u0026amp; Access Management) Understand authorization model in AWS – from basic to advanced. Successfully created: IAM Role for EC2 (permissions to read S3, Secrets Manager). IAM User for dev/test environment. Custom Managed Policy for HRM project. Applied Least Privilege standard to all resources. 📦 AWS S3 – Integration into HRM System Created dedicated S3 Bucket for project (avatar + employee records). Configured: Bucket Policy CORS Encryption (SSE-S3) Spring Boot integration: Upload/delete files Generate Pre-signed URL Validate file type \u0026amp; size Ensured personal data stored securely following Private Bucket model. 🗝 AWS Secrets Manager Store database passwords, JWT secret key. Integrated Java AWS SDK to load secrets when application runs. Removed hard-coded credentials → increased security. ✉️ AWS SES (Simple Email Service) Send email notifications for HRM events: leave requests, account creation, password reset. Configured domain verification \u0026amp; create identities (sandbox mode). 📊 Basic Logging \u0026amp; Monitoring Integrated CloudWatch Logs in EC2. Push Spring Boot application logs to CloudWatch to monitor errors \u0026amp; performance. Created separate log group for each environment. 📌 Personal Notes: This week is one of the most important weeks, as I began connecting the backend with real services on AWS. Content like IAM, S3, Secrets Manager and SES helped me better understand how enterprises design systems that are both secure and scalable. Integrating these services into the HRM project provided direct experience with cloud application development in practice.\n"},{"uri":"https://thienluhoan.github.io/workshop-template/1-worklog/1.11-week11/","title":"Week 11 Worklog","tags":[],"description":"","content":"🎯 Week 11 Objectives: Complete remaining modules of Human Resource Management system (HRM). Integrate AWS SES to send automated emails for HR operations. Use AWS CloudWatch to monitor, log and alert. Perform Integration Testing, prepare to move to full system testing phase. 📌 Tasks to be carried out this week: Day Task Start Date Completion Date Documentation 2 - Summarize feedback from mentor week 10 - Update IAM Role \u0026amp; S3 Policy to standard 15/09/2025 15/09/2025 3 - Integrate SES into HRM system: + Send leave notification emails + Send onboarding emails for new employees + Send OTP / password reset 16/09/2025 16/09/2025 AWS SES Docs 4 - Set up CloudWatch Logs for EC2 application - Create Log Group / Metric Filter - Try creating simple alert (Error \u0026gt; 10 events) 17/09/2025 17/09/2025 AWS CloudWatch Docs 5 - Optimize HRM backend: + Query payroll + Fix attendance logic + Improve API response time 18/09/2025 18/09/2025 6 - Write Integration Test (JUnit + Mockito) for: + Payroll Service + Attendance Service + Leave Management 19/09/2025 19/09/2025 Testing Docs ✅ Week 11 Achievements: ✉️ AWS SES – Email Automation Successfully integrated email sending features from HRM system:\nLeave notification emails for management. Account \u0026amp; password emails for new employees. OTP / password reset emails. Understand and apply: Domain verification Identity management Sandbox mode (test environment) Created reusable, lightweight and scalable email service.\n📊 AWS CloudWatch – Logging \u0026amp; Monitoring Push logs from EC2 \u0026amp; Spring Boot to CloudWatch. Created separate Log Group: hrm-backend-prod-logs. Set up Metric Filter to detect: ERROR WARN Unauthorized access Created CloudWatch Alarm to send email when error count exceeds threshold. ⚙️ Backend Optimization (HRM Project) Reduced payroll processing time by optimizing SQL + Stream API. Fixed attendance tracking logic that was double-checking. Improved API performance thanks to local caching. Standardized DTO + Response Model. 🧪 Integration Testing (Spring Boot) Wrote tests for main modules:\nModule Progress Payroll Service ✔ Completed Attendance Service ✔ Completed Leave Management ✔ Completed Authentication + JWT ⏳ In progress (next week 12) Results:\nAchieved coverage ~ 65%. Detected and fixed 3 logic errors due to missing null checks \u0026amp; missing fields. 📌 Personal Notes: Week 11 helped me understand the process of completing backend in real cloud environment: from email service, logging, monitoring to integration testing. Although workload was quite heavy, thanks to clear task division and mentor support, I successfully deployed\n"},{"uri":"https://thienluhoan.github.io/workshop-template/1-worklog/1.12-week12/","title":"Week 12 Worklog","tags":[],"description":"","content":"Week 12 Objectives: Summarize entire internship process. Complete remaining project tasks. Standardize documentation, internship report, worklog. Review entire system to prepare for handover. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Review all completed tasks in sprints - Check remaining errors in system 01/09/2025 01/09/2025 3 - Complete business documentation, technical documentation - Add missing Worklog for remaining weeks 02/09/2025 02/09/2025 4 - Optimize API, check logs, validate input - Write handover guide for team 03/09/2025 03/09/2025 5 - Prepare final report - Summarize knowledge, lessons learned throughout 12 weeks 04/09/2025 04/09/2025 6 - Demo entire system again - Fix minor issues that arise - Submit complete report + Worklog 05/09/2025 05/09/2025 Week 12 Achievements: Completed all project tasks, including:\nBackend for Human Resource Management system (Spring Boot) Python facial recognition integration API processing, validation, logging, basic security Completed entire 12-week Worklog and handover documentation.\nSummarized and extracted important lessons:\nTeamwork process in enterprise environment. Task division by sprint, deadline management. How to write technical documentation and communicate with mentor/trainer. Conducted internal demo:\nPresented system architecture Main API operation process Facial recognition feature (Python + OpenCV) Ready to hand over entire source code, documentation and project environment.\nCompleted all FCJ internship requirements.\n"},{"uri":"https://thienluhoan.github.io/workshop-template/","title":"Internship Report","tags":[],"description":"","content":"Internship Report Student Information: Full Name: Tran Dinh Son\nPhone Number: 0389920644\nEmail: Sontdse184005@fpt.edu.vn\nUniversity: FPT University\nMajor: Information Technology\nClass: AWS082025\nInternship Company: Amazon Web Services Vietnam Co.\nInternship Position: FCJ Cloud Intern\nInternship Duration: From 12/08/2025 to 12/11/2025\nReport Content Worklog Proposal Translated Blogs Events Participated Workshop Self-evaluation Sharing and Feedback "},{"uri":"https://thienluhoan.github.io/workshop-template/categories/","title":"Categories","tags":[],"description":"","content":""},{"uri":"https://thienluhoan.github.io/workshop-template/tags/","title":"Tags","tags":[],"description":"","content":""}]